"""
ç´ æå¤„ç†æœåŠ¡
Phase 2: é›†æˆè§†é¢‘å¤„ç†ã€AIåˆ†æå’Œæ•°æ®åº“å­˜å‚¨
"""

import os
import tempfile
from typing import Dict, Any, List
from sqlalchemy.orm import Session
from fastapi import UploadFile
from services.video_processor import VideoProcessor
from services.gemini_client import GeminiClient
from services.database_service import DatabaseService
from services.audio_transcriber import AudioTranscriber
from services.visual_processor import VisualProcessor
from services.memory_store import MemoryStore
from models.base import AssetCreate
import uuid
import asyncio

class AssetProcessor:
    
    def __init__(self, db: Session):
        self.db = db
        self.db_service = DatabaseService(db)
        self.video_processor = VideoProcessor()
        self.gemini_client = GeminiClient()
        self.audio_transcriber = AudioTranscriber()
        self.visual_processor = VisualProcessor()
        self.memory_store = MemoryStore()  # Initialize Vector Memory
    
    async def process_uploaded_file(self, file: UploadFile, project_id: str) -> Dict[str, Any]:
        """
        å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶ - å®Œæ•´æµç¨‹
        """
        
        try:
            # 1. åˆ›å»ºèµ„äº§è®°å½•
            asset_data = AssetCreate(
                project_id=project_id,
                filename=file.filename,
                mime_type=file.content_type or "application/octet-stream",
                source="upload"
            )
            
            asset = await self.db_service.create_asset(asset_data)
            
            # 2. ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°ä¸´æ—¶ä½ç½®
            temp_file_path = await self._save_uploaded_file(file)
            
            # 3. æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
            await self.db_service.update_asset_status(asset.id, "processing", 10)
            
            # 4. æ£€æŸ¥æ–‡ä»¶ç±»å‹å¹¶å¤„ç†
            if self._is_video_file(file.filename):
                result = await self._process_video_asset(asset.id, temp_file_path)
            else:
                result = await self._process_image_asset(asset.id, temp_file_path)
            
            if result["status"] == "success":
                # 5. æ›´æ–°èµ„äº§è·¯å¾„ä¿¡æ¯
                paths = result.get("paths", {})
                await self.db_service.update_asset_paths(
                    asset.id,
                    file_path=paths.get("original"),
                    proxy_path=paths.get("proxy"),
                    thumbnail_path=paths.get("thumbnail")
                )
                
                # 6. æ›´æ–°çŠ¶æ€ä¸ºå®Œæˆ
                await self.db_service.update_asset_status(asset.id, "completed", 100)
                
                return {
                    "status": "success",
                    "asset_id": asset.id,
                    "processing_result": result
                }
            else:
                # å¤„ç†å¤±è´¥
                await self.db_service.update_asset_status(asset.id, "error", 0)
                return {
                    "status": "error",
                    "asset_id": asset.id,
                    "error": result.get("error", "Unknown processing error")
                }
                
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "trace_id": str(uuid.uuid4())
            }
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if 'temp_file_path' in locals() and os.path.exists(temp_file_path):
                os.unlink(temp_file_path)
    
    async def _process_video_asset(self, asset_id: str, file_path: str) -> Dict[str, Any]:
        """å¤„ç†è§†é¢‘èµ„äº§"""
        
        try:
            # 1. è§†é¢‘å¤„ç† (ä»£ç†æ–‡ä»¶ã€ç¼©ç•¥å›¾ã€éŸ³é¢‘æå–)
            await self.db_service.update_asset_status(asset_id, "processing", 30)
            video_result = await self.video_processor.process_video(asset_id, file_path)
            
            if video_result["status"] != "success":
                return video_result
            
            # 2. è§†è§‰ç‰¹å¾æå– (é¢„å¤„ç† - æå–å…³é”®å¸§å›¾ç‰‡)
            await self.db_service.update_asset_status(asset_id, "processing", 40)
            
            visual_result = None
            pil_images = []
            
            if video_result.get("paths", {}).get("original"):
                original_path = video_result["paths"]["original"]
                # è¯·æ±‚è¿”å›PILå›¾ç‰‡ç”¨äºAIåˆ†æ
                visual_result = await self.visual_processor.extract_visual_features(
                    original_path, 
                    asset_id, 
                    return_images=True
                )
                
                if visual_result["status"] == "success":
                    # è·å–å›¾ç‰‡åˆ—è¡¨
                    pil_images = visual_result.get("images", [])
                    # å­˜å‚¨è§†è§‰åˆ†ææ•°æ®åˆ°æ•°æ®åº“
                    visual_data = visual_result["visual_analysis"]
                    self.db_service.store_visual_data(asset_id, visual_data)

            # 3. AIå†…å®¹åˆ†æ (Multimodal Vision)
            await self.db_service.update_asset_status(asset_id, "processing", 60)
            
            # è·å–èµ„äº§ä¿¡æ¯ç”¨äºAIåˆ†æ
            asset = self.db_service.get_asset(asset_id)
            description = f"è§†é¢‘æ–‡ä»¶: {asset.filename}"
            
            # ä¼ å…¥å…³é”®å¸§å›¾ç‰‡è¿›è¡Œå¤šæ¨¡æ€åˆ†æ
            ai_result = await self.gemini_client.analyze_video_content(
                filename=asset.filename,
                description=description,
                images=pil_images
            )
            
            # 4. åˆ›å»ºsegmentså’Œå‘é‡ç´¢å¼•
            await self.db_service.update_asset_status(asset_id, "processing", 80)
            
            segments_for_vectors = []
            
            if ai_result["status"] == "success":
                ai_data = ai_result["data"]
                
                # åˆ›å»ºsegmentè®°å½•
                for segment_data in ai_data.get("segments", []):
                    segment = await self.db_service.create_asset_segment(
                        asset_id=asset_id,
                        start_time=segment_data.get("start_time", 0),
                        end_time=segment_data.get("end_time", 10),
                        description=segment_data.get("description", ""),
                        tags=segment_data.get("tags", {})
                    )
                    
                    # å‡†å¤‡å‘é‡åŒ–æ•°æ®
                    segments_for_vectors.append({
                        "id": segment.id,
                        "description": segment.description,
                        "tags": {
                            "emotions": segment.emotion_tags or [],
                            "scenes": segment.scene_tags or [],
                            "actions": segment.action_tags or [],
                            "cinematography": segment.cinematography_tags or []
                        }
                    })
                
                # 4. éŸ³é¢‘è½¬å½•å¤„ç†
            await self.db_service.update_asset_status(asset_id, "processing", 80)
            
            transcription_result = None
            if video_result.get("paths", {}).get("audio"):
                audio_path = video_result["paths"]["audio"]
                transcription_result = await self.audio_transcriber.transcribe_audio(audio_path, asset_id)
                
                if transcription_result["status"] == "success":
                    # å­˜å‚¨è½¬å½•æ•°æ®åˆ°æ•°æ®åº“
                    transcription_data = transcription_result["transcription"]
                    self.db_service.store_transcription_data(asset_id, transcription_data)
            
            # 5. [Moved] è§†è§‰ç‰¹å¾æå–å·²åœ¨æ­¥éª¤2å®Œæˆ
            # æ­¤å‰é€»è¾‘å·²åˆå¹¶åˆ°ä¸Šæ–¹
            
            # 6. åˆ›å»ºå‘é‡ç´¢å¼• (Inject into Memory Store)
            await self.db_service.update_asset_status(asset_id, "processing", 90)
            
            # Persist Visual Vectors to ChromaDB
            if visual_result and visual_result["status"] == "success":
                # Check if we have raw feature vectors (usually 512-float list)
                # Note: VisualProcessor need to return 'feature_vector' in visual_result
                # If not available, we skip for now or compute here. 
                # Assuming VisualProcessor.extract_visual_features returns 'feature_vector' for the whole clip or per keyframe.
                
                # For MVP, let's look for a global feature vector in visual_result
                feature_vector = visual_result.get("visual_analysis", {}).get("feature_vector")
                
                if feature_vector:
                    # Metadata for retrieval
                    metadata = {
                        "asset_id": asset_id,
                        "filename": asset.filename,
                        "type": "visual",
                        "project_id": asset.project_id
                    }
                    self.memory_store.add_memory(
                        asset_id=f"{asset_id}_visual_global",
                        vector=feature_vector,
                        metadata=metadata
                    )
                    # print(f"ğŸ§  [Memory] Ingested visual vector for {asset.filename}")
            
            # Keep existing logic as fallback or parallel if needed, but primarily reliance on MemoryStore now.
            if segments_for_vectors:
                 # Legacy or Text-based vector logic (optional to keep or migrate)
                 pass
            
            return {
                "status": "success",
                "paths": video_result["paths"],
                "video_info": video_result.get("video_info", {}),
                "ai_analysis": ai_result,
                "transcription": transcription_result,
                "visual_analysis": visual_result
            }
            
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }
    
    async def _process_image_asset(self, asset_id: str, file_path: str) -> Dict[str, Any]:
        """å¤„ç†å›¾ç‰‡èµ„äº§ - ç®€åŒ–ç‰ˆæœ¬"""
        
        try:
            # å¯¹äºå›¾ç‰‡ï¼Œåªéœ€è¦ç§»åŠ¨æ–‡ä»¶å’Œç”Ÿæˆç¼©ç•¥å›¾
            import shutil
            
            asset_root = self.video_processor.asset_root
            original_path = f"{asset_root}/originals/{asset_id}.jpg"
            thumbnail_path = f"{asset_root}/thumbnails/{asset_id}_thumb.jpg"
            
            # ç§»åŠ¨åŸå§‹æ–‡ä»¶
            shutil.move(file_path, original_path)
            
            # å¤åˆ¶ä½œä¸ºç¼©ç•¥å›¾ (ç®€åŒ–å¤„ç†)
            shutil.copy2(original_path, thumbnail_path)
            
            return {
                "status": "success",
                "paths": {
                    "original": original_path,
                    "thumbnail": thumbnail_path
                }
            }
            
        except Exception as e:
            return {
                "status": "error", 
                "error": str(e)
            }
    
    async def _save_uploaded_file(self, file: UploadFile) -> str:
        """ä¿å­˜ä¸Šä¼ æ–‡ä»¶åˆ°ä¸´æ—¶ä½ç½®"""
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        suffix = os.path.splitext(file.filename)[1] if file.filename else ""
        temp_fd, temp_path = tempfile.mkstemp(suffix=suffix)
        
        try:
            # è¯»å–å¹¶å†™å…¥æ–‡ä»¶å†…å®¹
            content = await file.read()
            with os.fdopen(temp_fd, 'wb') as temp_file:
                temp_file.write(content)
            
            return temp_path
            
        except Exception:
            # å¦‚æœå‡ºé”™ï¼Œæ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.close(temp_fd)
            if os.path.exists(temp_path):
                os.unlink(temp_path)
            raise
    
    def _is_video_file(self, filename: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºè§†é¢‘æ–‡ä»¶"""
        if not filename:
            return False
        
        video_extensions = {'.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv', '.webm'}
        ext = os.path.splitext(filename)[1].lower()
        return ext in video_extensions
    
    async def get_asset_status(self, asset_id: str) -> Dict[str, Any]:
        """è·å–èµ„äº§å¤„ç†çŠ¶æ€"""
        
        asset = await self.db_service.get_asset(asset_id)
        if not asset:
            return {
                "status": "error",
                "message": "Asset not found"
            }
        
        # è·å–segments
        segments = await self.db_service.get_asset_segments(asset_id)
        
        segment_list = []
        for segment in segments:
            segment_list.append({
                "id": segment.id,
                "start_time": segment.start_time,
                "end_time": segment.end_time,
                "description": segment.description,
                "tags": {
                    "emotions": segment.emotion_tags or [],
                    "scenes": segment.scene_tags or [],
                    "actions": segment.action_tags or [],
                    "cinematography": segment.cinematography_tags or []
                }
            })
        
        # è·å–è½¬å½•æ•°æ®
        transcription_data = self.db_service.get_transcription_data(asset_id)
        
        # è·å–è§†è§‰åˆ†ææ•°æ®
        visual_data = self.db_service.get_visual_data(asset_id)
        
        return {
            "status": asset.processing_status,
            "progress": asset.processing_progress,
            "proxy_url": f"/assets/proxies/{asset_id}_proxy.mp4" if asset.proxy_path else None,
            "thumbnail_url": f"/assets/thumbnails/{asset_id}_thumb.jpg" if asset.thumbnail_path else None,
            "segments": segment_list,
            "transcription": transcription_data,
            "visual_analysis": visual_data
        }
    
    def _prepare_transcription_vectors(self, transcription_data: Dict[str, Any]) -> List[Dict]:
        """å‡†å¤‡è½¬å½•æ–‡æœ¬çš„å‘é‡åŒ–æ•°æ®"""
        
        vector_segments = []
        
        for segment in transcription_data.get("segments", []):
            # ä¸ºæ¯ä¸ªè½¬å½•ç‰‡æ®µåˆ›å»ºå‘é‡æ•°æ®
            vector_segment = {
                "id": f"transcript_{segment['id']}",
                "description": segment["text"],
                "tags": {
                    "content_type": ["transcript", "audio"],
                    "language": [transcription_data.get("language", "unknown")],
                    "confidence": [f"confidence_{int(segment['confidence'] * 100)}"],
                    "time_range": [f"{segment['start_time']:.1f}s-{segment['end_time']:.1f}s"]
                }
            }
            vector_segments.append(vector_segment)
        
        # å¦‚æœè½¬å½•æ–‡æœ¬è¾ƒé•¿ï¼Œä¹Ÿä¸ºå…¨æ–‡åˆ›å»ºä¸€ä¸ªå‘é‡
        full_text = transcription_data.get("full_text", "")
        if len(full_text) > 50:  # åªæœ‰è¶³å¤Ÿé•¿çš„æ–‡æœ¬æ‰åˆ›å»ºå…¨æ–‡å‘é‡
            vector_segments.append({
                "id": "transcript_full",
                "description": full_text,
                "tags": {
                    "content_type": ["transcript", "audio", "full_text"],
                    "language": [transcription_data.get("language", "unknown")],
                    "duration": [f"{transcription_data.get('duration', 0):.1f}s"]
                }
            })
        
        return vector_segments
    
    def _prepare_visual_vectors(self, visual_data: Dict[str, Any]) -> List[Dict]:
        """å‡†å¤‡è§†è§‰ç‰¹å¾çš„å‘é‡åŒ–æ•°æ®"""
        
        vector_segments = []
        
        # ä¸ºæ¯ä¸ªå…³é”®å¸§åˆ›å»ºå‘é‡æ•°æ®
        keyframes = visual_data.get("keyframes", [])
        for keyframe in keyframes:
            timestamp = keyframe.get("timestamp", 0)
            analysis = keyframe.get("analysis", {})
            
            # æ„å»ºè§†è§‰æè¿°
            visual_description_parts = []
            
            # äº®åº¦å’Œå¯¹æ¯”åº¦æè¿°
            brightness = analysis.get("brightness", 0)
            contrast = analysis.get("contrast", 0)
            
            if brightness > 150:
                visual_description_parts.append("æ˜äº®ç”»é¢")
            elif brightness < 80:
                visual_description_parts.append("æ˜æš—ç”»é¢")
            else:
                visual_description_parts.append("æ­£å¸¸äº®åº¦")
            
            if contrast > 50:
                visual_description_parts.append("é«˜å¯¹æ¯”åº¦")
            elif contrast < 20:
                visual_description_parts.append("ä½å¯¹æ¯”åº¦")
            
            # è‰²å½©æè¿°
            color_balance = analysis.get("color_balance", {})
            if color_balance:
                red = color_balance.get("red", 0)
                blue = color_balance.get("blue", 0)
                if red > blue + 20:
                    visual_description_parts.append("æš–è‰²è°ƒ")
                elif blue > red + 20:
                    visual_description_parts.append("å†·è‰²è°ƒ")
            
            # å¤æ‚åº¦æè¿°
            complexity = analysis.get("estimated_complexity", "medium")
            visual_description_parts.append(f"{complexity}å¤æ‚åº¦ç”»é¢")
            
            description = " ".join(visual_description_parts)
            
            vector_segment = {
                "id": f"visual_{keyframe.get('frame_index', 0)}",
                "description": description,
                "tags": {
                    "content_type": ["visual", "keyframe"],
                    "timestamp": [f"{timestamp:.1f}s"],
                    "brightness": [self._categorize_brightness(brightness)],
                    "complexity": [complexity],
                    "visual_features": ["color_analysis", "composition"]
                }
            }
            vector_segments.append(vector_segment)
        
        # ä¸ºæ•´ä½“è§†è§‰ç‰¹å¾åˆ›å»ºä¸€ä¸ªå‘é‡
        visual_summary = visual_data.get("visual_description", {}).get("visual_summary", {})
        if visual_summary:
            overall_description_parts = []
            
            brightness_level = visual_summary.get("brightness_level", "normal")
            color_tone = visual_summary.get("color_tone", "neutral")
            complexity = visual_summary.get("visual_complexity", "medium")
            lighting = visual_summary.get("lighting_quality", "normal")
            
            overall_description_parts.extend([
                f"{brightness_level}äº®åº¦",
                f"{color_tone}è‰²è°ƒ",
                f"{complexity}å¤æ‚åº¦",
                f"{lighting}å…‰çº¿"
            ])
            
            vector_segments.append({
                "id": "visual_overall",
                "description": " ".join(overall_description_parts),
                "tags": {
                    "content_type": ["visual", "overall"],
                    "brightness_level": [brightness_level],
                    "color_tone": [color_tone],
                    "visual_complexity": [complexity],
                    "lighting_quality": [lighting]
                }
            })
        
        return vector_segments
    
    def _categorize_brightness(self, brightness: float) -> str:
        """åˆ†ç±»äº®åº¦çº§åˆ«"""
        if brightness > 180:
            return "very_bright"
        elif brightness > 120:
            return "bright"
        elif brightness > 80:
            return "normal"
        elif brightness > 40:
            return "dark"
        else:
            return "very_dark"