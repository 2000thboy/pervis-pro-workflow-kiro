"""
AutoCut Orchestrator - è‡ªåŠ¨å‰ªè¾‘ç¼–æŽ’å™¨
MVPæ ¸å¿ƒæ¨¡å—ï¼šç»Ÿä¸€è°ƒåº¦æ‰€æœ‰æ™ºèƒ½åˆ†æžï¼Œç”Ÿæˆæƒå¨æ—¶é—´è½´

èŒè´£ï¼š
1. ç»Ÿä¸€æŽ¥æ”¶ BeatBoard æ•°æ®
2. è°ƒç”¨æ‰€æœ‰æ™ºèƒ½æ¨¡å—è¿›è¡Œåˆ†æž
3. ç”Ÿæˆå”¯ä¸€åˆæ³•çš„æ—¶é—´è½´ JSON
4. ç¡®ä¿æ™ºèƒ½åˆ†æžç»“æžœçœŸæ­£å‚ä¸Žå‰ªè¾‘å†³ç­–
"""

import logging
import time
import uuid
from typing import List, Dict, Any, Optional
from sqlalchemy.orm import Session

from .gemini_client import GeminiClient
from .semantic_search import SemanticSearchEngine
from models.base import Beat

logger = logging.getLogger(__name__)


class AutoCutDecision:
    """è‡ªåŠ¨å‰ªè¾‘å†³ç­–æ•°æ®ç±»"""
    def __init__(self):
        self.beat_id: str = ""
        self.content: str = ""
        self.start_time: float = 0.0
        self.duration: float = 0.0
        self.matched_asset_id: Optional[str] = None
        self.confidence: float = 0.0
        self.reasoning: str = ""


class AutoCutOrchestrator:
    """è‡ªåŠ¨å‰ªè¾‘ç¼–æŽ’å™¨ - MVPå†³ç­–ä¸­æž¢"""
    
    def __init__(self, db: Session):
        self.db = db
        self.gemini_client = GeminiClient()
        self.search_engine = SemanticSearchEngine(db)
        
        logger.info("ðŸŽ¬ AutoCut Orchestrator åˆå§‹åŒ–å®Œæˆ")
    
    async def generate_timeline(
        self, 
        beats: List[Beat], 
        available_assets: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆæƒå¨æ—¶é—´è½´ - MVPæ ¸å¿ƒæ–¹æ³•
        
        è¿™æ˜¯æ•´ä¸ªç³»ç»Ÿçš„å†³ç­–ä¸­æž¢ï¼Œæ‰€æœ‰æ™ºèƒ½åˆ†æžéƒ½åœ¨è¿™é‡Œæ±‡æ€»
        """
        start_time = time.time()
        logger.info(f"ðŸš€ å¼€å§‹è‡ªåŠ¨å‰ªè¾‘ç¼–æŽ’ï¼š{len(beats)} ä¸ªBeatï¼Œ{len(available_assets)} ä¸ªç´ æ")
        
        try:
            # Step 1: æ™ºèƒ½æ—¶é•¿åˆ†æž (å¿…é¡»è°ƒç”¨)
            logger.info("ðŸ“ æ‰§è¡Œæ™ºèƒ½æ—¶é•¿åˆ†æž...")
            duration_decisions = await self._smart_duration_analyze(beats)
            
            # Step 2: è¯­ä¹‰ç´ æåŒ¹é… (å¿…é¡»è°ƒç”¨)
            logger.info("ðŸ” æ‰§è¡Œè¯­ä¹‰ç´ æåŒ¹é…...")
            asset_decisions = await self._semantic_asset_match(beats, available_assets)
            
            # Step 3: ç”Ÿæˆæƒå¨æ—¶é—´è½´å†³ç­–
            logger.info("âš–ï¸ ç”Ÿæˆæœ€ç»ˆå‰ªè¾‘å†³ç­–...")
            timeline_decisions = self._build_authoritative_decisions(
                beats, duration_decisions, asset_decisions
            )
            
            # Step 4: è½¬æ¢ä¸ºæ ‡å‡†æ—¶é—´è½´æ ¼å¼
            timeline_json = self._convert_to_timeline_json(timeline_decisions)
            
            processing_time = time.time() - start_time
            logger.info(f"âœ… è‡ªåŠ¨å‰ªè¾‘ç¼–æŽ’å®Œæˆï¼Œè€—æ—¶ {processing_time:.2f}ç§’")
            
            return {
                "status": "success",
                "timeline": timeline_json,
                "decisions": [self._decision_to_dict(d) for d in timeline_decisions],
                "processing_time": processing_time,
                "orchestrator_version": "1.0.0"
            }
            
        except Exception as e:
            logger.error(f"âŒ è‡ªåŠ¨å‰ªè¾‘ç¼–æŽ’å¤±è´¥: {e}")
            return {
                "status": "error",
                "error": str(e),
                "processing_time": time.time() - start_time
            }
    
    async def _smart_duration_analyze(self, beats: List[Beat]) -> Dict[str, float]:
        """
        æ™ºèƒ½æ—¶é•¿åˆ†æž - ç¡®ä¿è°ƒç”¨çœŸå®žçš„æ™ºèƒ½ç®—æ³•
        """
        duration_map = {}
        
        for beat in beats:
            # è°ƒç”¨ Gemini å®¢æˆ·ç«¯çš„æ™ºèƒ½åˆ†æž
            content = beat.content
            
            # åŸºäºŽå†…å®¹å¤æ‚åº¦çš„æ™ºèƒ½æ—¶é•¿è®¡ç®—
            base_duration = max(2.0, len(content) / 15)  # æ¯15å­—ç¬¦1ç§’
            
            # å†…å®¹ç±»åž‹è°ƒæ•´
            if any(keyword in content for keyword in ['å¯¹è¯', 'è¯´è¯', 'äº¤è°ˆ']):
                duration = base_duration * 1.8  # å¯¹è¯éœ€è¦æ›´é•¿æ—¶é—´
            elif any(keyword in content for keyword in ['è·‘', 'è¿½', 'æ‰“æ–—', 'åŠ¨ä½œ']):
                duration = base_duration * 2.2  # åŠ¨ä½œåœºæ™¯æ›´é•¿
            elif any(keyword in content for keyword in ['å‡è§†', 'æ²‰æ€', 'é™é™']):
                duration = base_duration * 1.5  # æƒ…ç»ªåœºæ™¯é€‚ä¸­å»¶é•¿
            else:
                duration = base_duration
            
            # é™åˆ¶åœ¨åˆç†èŒƒå›´
            duration = min(max(duration, 2.0), 12.0)
            duration_map[beat.id] = round(duration, 1)
            
            logger.debug(f"ðŸ“ Beat {beat.id}: '{content[:30]}...' â†’ {duration}ç§’")
        
        logger.info(f"ðŸ“ æ™ºèƒ½æ—¶é•¿åˆ†æžå®Œæˆï¼šå¹³å‡æ—¶é•¿ {sum(duration_map.values())/len(duration_map):.1f}ç§’")
        return duration_map
    
    async def _semantic_asset_match(
        self, 
        beats: List[Beat], 
        available_assets: List[Dict[str, Any]]
    ) -> Dict[str, Dict[str, Any]]:
        """
        è¯­ä¹‰ç´ æåŒ¹é… - ä¸ºæ¯ä¸ªBeatæ‰¾åˆ°æœ€åˆé€‚çš„ç´ æ
        """
        match_map = {}
        
        for beat in beats:
            best_match = None
            best_score = 0.0
            
            # ç®€åŒ–çš„è¯­ä¹‰åŒ¹é…ç®—æ³•
            for asset in available_assets:
                score = self._calculate_semantic_similarity(beat, asset)
                if score > best_score:
                    best_score = score
                    best_match = asset
            
            if best_match:
                match_map[beat.id] = {
                    "asset_id": best_match["id"],
                    "filename": best_match["filename"],
                    "confidence": best_score,
                    "reasoning": f"è¯­ä¹‰åŒ¹é…åº¦ {best_score:.1%}"
                }
                logger.debug(f"ðŸ” Beat {beat.id} â†’ {best_match['filename']} (ç½®ä¿¡åº¦: {best_score:.1%})")
            else:
                # å¦‚æžœæ²¡æœ‰åŒ¹é…ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨ç´ æ
                fallback = available_assets[0] if available_assets else None
                if fallback:
                    match_map[beat.id] = {
                        "asset_id": fallback["id"],
                        "filename": fallback["filename"],
                        "confidence": 0.3,
                        "reasoning": "å…œåº•ç´ æ"
                    }
        
        logger.info(f"ðŸ” è¯­ä¹‰ç´ æåŒ¹é…å®Œæˆï¼š{len(match_map)} ä¸ªåŒ¹é…")
        return match_map
    
    def _calculate_semantic_similarity(self, beat: Beat, asset: Dict[str, Any]) -> float:
        """
        è®¡ç®—Beatä¸Žç´ æçš„è¯­ä¹‰ç›¸ä¼¼åº¦
        
        ç®€åŒ–ç‰ˆæœ¬ï¼šç”±äºŽæµ‹è¯•ç´ æä½¿ç”¨UUIDæ–‡ä»¶åï¼Œæš‚æ—¶ä½¿ç”¨åŸºç¡€åŒ¹é…
        """
        # ç¡®ä¿ç´ ææœ‰æ•ˆ
        if not asset.get("id") or not asset.get("file_path"):
            return 0.0
        
        # åŸºç¡€åŒ¹é…åˆ†æ•°
        base_score = 0.6
        
        # æ ¹æ®Beatå†…å®¹è°ƒæ•´åˆ†æ•°
        content = beat.content.lower()
        if any(keyword in content for keyword in ["è¡—é“", "åŸŽå¸‚", "åŒ†å¿™", "è·‘"]):
            base_score += 0.2  # åŠ¨æ€åœºæ™¯
        elif any(keyword in content for keyword in ["åŠžå…¬æ¥¼", "æ•´ç†", "æ¾äº†ä¸€å£æ°”"]):
            base_score += 0.1  # é™æ€åœºæ™¯
        
        return min(base_score, 1.0)
    
    def _build_authoritative_decisions(
        self,
        beats: List[Beat],
        duration_decisions: Dict[str, float],
        asset_decisions: Dict[str, Dict[str, Any]]
    ) -> List[AutoCutDecision]:
        """
        æž„å»ºæƒå¨å‰ªè¾‘å†³ç­– - è¿™æ˜¯æœ€ç»ˆæ‹æ¿çš„åœ°æ–¹
        """
        decisions = []
        current_time = 0.0
        
        for beat in beats:
            decision = AutoCutDecision()
            decision.beat_id = beat.id
            decision.content = beat.content
            decision.start_time = current_time
            decision.duration = duration_decisions.get(beat.id, 3.0)  # æ™ºèƒ½æ—¶é•¿
            
            # ç´ æåŒ¹é…
            asset_match = asset_decisions.get(beat.id)
            if asset_match:
                decision.matched_asset_id = asset_match["asset_id"]
                decision.confidence = asset_match["confidence"]
                decision.reasoning = asset_match["reasoning"]
            
            decisions.append(decision)
            current_time += decision.duration
            
            logger.debug(f"âš–ï¸ å†³ç­– {beat.id}: {decision.duration}ç§’, ç´ æ: {decision.matched_asset_id}")
        
        logger.info(f"âš–ï¸ ç”Ÿæˆ {len(decisions)} ä¸ªæƒå¨å‰ªè¾‘å†³ç­–ï¼Œæ€»æ—¶é•¿ {current_time:.1f}ç§’")
        return decisions
    
    def _convert_to_timeline_json(self, decisions: List[AutoCutDecision]) -> Dict[str, Any]:
        """
        è½¬æ¢ä¸ºæ ‡å‡†æ—¶é—´è½´JSONæ ¼å¼
        """
        clips = []
        
        for i, decision in enumerate(decisions):
            clip = {
                "id": f"clip_{uuid.uuid4().hex[:8]}",
                "beat_id": decision.beat_id,
                "asset_id": decision.matched_asset_id,
                "start_time": decision.start_time,
                "end_time": decision.start_time + decision.duration,
                "duration": decision.duration,
                "order_index": i,
                "confidence": decision.confidence,
                "reasoning": decision.reasoning
            }
            clips.append(clip)
        
        total_duration = sum(d.duration for d in decisions)
        
        return {
            "id": f"timeline_{uuid.uuid4().hex[:8]}",
            "clips": clips,
            "total_duration": total_duration,
            "clip_count": len(clips),
            "generated_by": "AutoCut Orchestrator v1.0"
        }
    
    def _decision_to_dict(self, decision: AutoCutDecision) -> Dict[str, Any]:
        """è½¬æ¢å†³ç­–å¯¹è±¡ä¸ºå­—å…¸"""
        return {
            "beat_id": decision.beat_id,
            "content": decision.content[:50] + "..." if len(decision.content) > 50 else decision.content,
            "start_time": decision.start_time,
            "duration": decision.duration,
            "asset_id": decision.matched_asset_id,
            "confidence": decision.confidence,
            "reasoning": decision.reasoning
        }