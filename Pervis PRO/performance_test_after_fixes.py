#!/usr/bin/env python3
"""
P0/P1ä¿®å¤åçš„æ€§èƒ½æµ‹è¯•è„šæœ¬
éªŒè¯ä¿®å¤æ•ˆæœå¹¶ç”Ÿæˆè¯¦ç»†æµ‹è¯•æŠ¥å‘Š
"""

import asyncio
import json
import time
import requests
import sys
from pathlib import Path
from typing import Dict, List, Any
import statistics
import subprocess
import os

class PerformanceTestSuite:
    """æ€§èƒ½æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self):
        self.base_url = "http://localhost:8000"
        self.frontend_url = "http://localhost:3000"
        self.test_results = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "test_environment": {
                "backend_url": self.base_url,
                "frontend_url": self.frontend_url,
                "python_version": sys.version,
                "platform": sys.platform
            },
            "performance_tests": {},
            "comparison": {},
            "summary": {
                "total_tests": 0,
                "passed_tests": 0,
                "failed_tests": 0,
                "performance_improvements": {}
            }
        }
        
        # ä¿®å¤å‰çš„åŸºå‡†æ•°æ®
        self.baseline_metrics = {
            "api_response_time": 2392.17,  # ms
            "frontend_load_time": 2033.0,  # ms
            "database_query_time": 150.0,  # ms (ä¼°ç®—)
            "bundle_size": 2.5  # MB (ä¼°ç®—)
        }
    
    def log_test(self, test_name: str, success: bool, metrics: Dict, message: str = ""):
        """è®°å½•æµ‹è¯•ç»“æœ"""
        self.test_results["performance_tests"][test_name] = {
            "success": success,
            "metrics": metrics,
            "message": message,
            "timestamp": time.strftime("%H:%M:%S")
        }
        
        self.test_results["summary"]["total_tests"] += 1
        if success:
            self.test_results["summary"]["passed_tests"] += 1
            print(f"âœ… {test_name}: {message}")
        else:
            self.test_results["summary"]["failed_tests"] += 1
            print(f"âŒ {test_name}: {message}")
        
        # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
        if metrics:
            for key, value in metrics.items():
                if isinstance(value, (int, float)):
                    print(f"   {key}: {value}")
    
    def test_api_response_time(self):
        """æµ‹è¯•APIå“åº”æ—¶é—´"""
        try:
            endpoints = [
                "/api/health",
                "/api/projects", 
                "/api/batch/queue/status",
                "/api/batch/stats"
            ]
            
            response_times = []
            successful_requests = 0
            
            for endpoint in endpoints:
                times = []
                for i in range(10):  # æ¯ä¸ªç«¯ç‚¹æµ‹è¯•10æ¬¡
                    start_time = time.time()
                    try:
                        response = requests.get(f"{self.base_url}{endpoint}", timeout=10)
                        end_time = time.time()
                        
                        if response.status_code == 200:
                            response_time = (end_time - start_time) * 1000  # ms
                            times.append(response_time)
                            successful_requests += 1
                    except Exception as e:
                        print(f"   è¯·æ±‚å¤±è´¥ {endpoint}: {e}")
                
                if times:
                    response_times.extend(times)
            
            if response_times:
                avg_response_time = statistics.mean(response_times)
                p95_response_time = statistics.quantiles(response_times, n=20)[18]  # 95th percentile
                min_response_time = min(response_times)
                max_response_time = max(response_times)
                
                # è®¡ç®—æ”¹è¿›å¹…åº¦
                baseline = self.baseline_metrics["api_response_time"]
                improvement = ((baseline - avg_response_time) / baseline) * 100
                
                metrics = {
                    "average_response_time_ms": round(avg_response_time, 2),
                    "p95_response_time_ms": round(p95_response_time, 2),
                    "min_response_time_ms": round(min_response_time, 2),
                    "max_response_time_ms": round(max_response_time, 2),
                    "successful_requests": successful_requests,
                    "total_requests": len(endpoints) * 10,
                    "improvement_percentage": round(improvement, 1)
                }
                
                # åˆ¤æ–­æ˜¯å¦è¾¾åˆ°ç›®æ ‡
                target_met = avg_response_time < 500
                message = f"å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.2f}ms (æ”¹è¿›: {improvement:.1f}%)"
                
                self.log_test("api_response_time", target_met, metrics, message)
                
                # è®°å½•æ”¹è¿›æ•°æ®
                self.test_results["comparison"]["api_response_time"] = {
                    "baseline": baseline,
                    "current": avg_response_time,
                    "improvement": improvement,
                    "target": 500,
                    "target_met": target_met
                }
                
                return avg_response_time
            else:
                self.log_test("api_response_time", False, {}, "æ‰€æœ‰APIè¯·æ±‚éƒ½å¤±è´¥")
                return -1
                
        except Exception as e:
            self.log_test("api_response_time", False, {}, f"APIå“åº”æ—¶é—´æµ‹è¯•å¼‚å¸¸: {str(e)}")
            return -1
    
    def test_database_connection_pool(self):
        """æµ‹è¯•æ•°æ®åº“è¿æ¥æ± æ•ˆæœ"""
        try:
            # å¹¶å‘æµ‹è¯•æ•°æ®åº“è¿æ¥
            concurrent_requests = 20
            start_time = time.time()
            
            def make_request():
                try:
                    response = requests.get(f"{self.base_url}/api/health", timeout=5)
                    return response.status_code == 200
                except:
                    return False
            
            # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œå¹¶å‘æµ‹è¯•
            from concurrent.futures import ThreadPoolExecutor
            with ThreadPoolExecutor(max_workers=concurrent_requests) as executor:
                futures = [executor.submit(make_request) for _ in range(concurrent_requests)]
                results = [future.result() for future in futures]
            
            end_time = time.time()
            total_time = (end_time - start_time) * 1000  # ms
            
            successful_requests = sum(results)
            success_rate = (successful_requests / concurrent_requests) * 100
            avg_time_per_request = total_time / concurrent_requests
            
            metrics = {
                "concurrent_requests": concurrent_requests,
                "successful_requests": successful_requests,
                "success_rate_percentage": round(success_rate, 1),
                "total_time_ms": round(total_time, 2),
                "avg_time_per_request_ms": round(avg_time_per_request, 2)
            }
            
            # è¿æ¥æ± æ•ˆæœè‰¯å¥½çš„æ ‡å‡†ï¼šæˆåŠŸç‡>95%ï¼Œå¹³å‡æ—¶é—´<100ms
            pool_effective = success_rate > 95 and avg_time_per_request < 100
            message = f"å¹¶å‘æˆåŠŸç‡: {success_rate:.1f}%, å¹³å‡æ—¶é—´: {avg_time_per_request:.2f}ms"
            
            self.log_test("database_connection_pool", pool_effective, metrics, message)
            
            return pool_effective
            
        except Exception as e:
            self.log_test("database_connection_pool", False, {}, f"è¿æ¥æ± æµ‹è¯•å¼‚å¸¸: {str(e)}")
            return False
    
    def test_cache_service(self):
        """æµ‹è¯•ç¼“å­˜æœåŠ¡æ•ˆæœ"""
        try:
            # æµ‹è¯•ç¼“å­˜å‘½ä¸­ç‡
            cache_test_endpoint = f"{self.base_url}/api/batch/stats"
            
            # ç¬¬ä¸€æ¬¡è¯·æ±‚ - ç¼“å­˜æœªå‘½ä¸­
            start_time = time.time()
            response1 = requests.get(cache_test_endpoint, timeout=10)
            first_request_time = (time.time() - start_time) * 1000
            
            # ç¬¬äºŒæ¬¡è¯·æ±‚ - åº”è¯¥å‘½ä¸­ç¼“å­˜
            start_time = time.time()
            response2 = requests.get(cache_test_endpoint, timeout=10)
            second_request_time = (time.time() - start_time) * 1000
            
            if response1.status_code == 200 and response2.status_code == 200:
                # è®¡ç®—ç¼“å­˜æ•ˆæœ
                cache_speedup = ((first_request_time - second_request_time) / first_request_time) * 100
                cache_effective = second_request_time < first_request_time * 0.8  # è‡³å°‘20%æå‡
                
                metrics = {
                    "first_request_time_ms": round(first_request_time, 2),
                    "second_request_time_ms": round(second_request_time, 2),
                    "cache_speedup_percentage": round(cache_speedup, 1),
                    "cache_effective": cache_effective
                }
                
                message = f"ç¼“å­˜åŠ é€Ÿ: {cache_speedup:.1f}% (ç¬¬ä¸€æ¬¡: {first_request_time:.2f}ms, ç¬¬äºŒæ¬¡: {second_request_time:.2f}ms)"
                
                self.log_test("cache_service", cache_effective, metrics, message)
                return cache_effective
            else:
                self.log_test("cache_service", False, {}, "ç¼“å­˜æµ‹è¯•è¯·æ±‚å¤±è´¥")
                return False
                
        except Exception as e:
            self.log_test("cache_service", False, {}, f"ç¼“å­˜æœåŠ¡æµ‹è¯•å¼‚å¸¸: {str(e)}")
            return False
    
    def test_response_compression(self):
        """æµ‹è¯•å“åº”å‹ç¼©æ•ˆæœ"""
        try:
            # æµ‹è¯•å¤§å“åº”çš„å‹ç¼©æ•ˆæœ
            test_endpoint = f"{self.base_url}/api/batch/tasks/history?limit=50"
            
            # ä¸å¯ç”¨å‹ç¼©çš„è¯·æ±‚
            headers_no_compression = {"Accept-Encoding": "identity"}
            response_no_compression = requests.get(test_endpoint, headers=headers_no_compression, timeout=10)
            
            # å¯ç”¨å‹ç¼©çš„è¯·æ±‚
            headers_with_compression = {"Accept-Encoding": "gzip, deflate"}
            response_with_compression = requests.get(test_endpoint, headers=headers_with_compression, timeout=10)
            
            if response_no_compression.status_code == 200 and response_with_compression.status_code == 200:
                uncompressed_size = len(response_no_compression.content)
                compressed_size = len(response_with_compression.content)
                
                # æ£€æŸ¥æ˜¯å¦çœŸçš„å‹ç¼©äº†
                is_compressed = "gzip" in response_with_compression.headers.get("Content-Encoding", "")
                compression_ratio = ((uncompressed_size - compressed_size) / uncompressed_size) * 100 if uncompressed_size > 0 else 0
                
                metrics = {
                    "uncompressed_size_bytes": uncompressed_size,
                    "compressed_size_bytes": compressed_size,
                    "compression_ratio_percentage": round(compression_ratio, 1),
                    "is_compressed": is_compressed,
                    "content_encoding": response_with_compression.headers.get("Content-Encoding", "none")
                }
                
                # å‹ç¼©æœ‰æ•ˆçš„æ ‡å‡†ï¼šå‹ç¼©æ¯”>30%æˆ–è€…å“åº”å¤´åŒ…å«gzip
                compression_effective = compression_ratio > 30 or is_compressed
                message = f"å‹ç¼©æ¯”: {compression_ratio:.1f}%, ç¼–ç : {response_with_compression.headers.get('Content-Encoding', 'none')}"
                
                self.log_test("response_compression", compression_effective, metrics, message)
                return compression_effective
            else:
                self.log_test("response_compression", False, {}, "å‹ç¼©æµ‹è¯•è¯·æ±‚å¤±è´¥")
                return False
                
        except Exception as e:
            self.log_test("response_compression", False, {}, f"å“åº”å‹ç¼©æµ‹è¯•å¼‚å¸¸: {str(e)}")
            return False
    
    def test_frontend_build_optimization(self):
        """æµ‹è¯•å‰ç«¯æ„å»ºä¼˜åŒ–æ•ˆæœ"""
        try:
            # æ£€æŸ¥æ„å»ºæ–‡ä»¶
            build_dir = Path("frontend/dist")
            if not build_dir.exists():
                # å°è¯•æ„å»º
                print("   æ­£åœ¨æ‰§è¡Œå‰ç«¯æ„å»º...")
                result = subprocess.run(
                    ["npm", "run", "build"], 
                    cwd="frontend", 
                    capture_output=True, 
                    text=True,
                    timeout=300
                )
                
                if result.returncode != 0:
                    self.log_test("frontend_build_optimization", False, {}, f"å‰ç«¯æ„å»ºå¤±è´¥: {result.stderr}")
                    return False
            
            # åˆ†ææ„å»ºç»“æœ
            js_files = list(build_dir.glob("**/*.js"))
            css_files = list(build_dir.glob("**/*.css"))
            
            total_js_size = sum(f.stat().st_size for f in js_files)
            total_css_size = sum(f.stat().st_size for f in css_files)
            total_size = total_js_size + total_css_size
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ä»£ç åˆ†å‰²
            chunk_files = [f for f in js_files if "chunk" in f.name or "vendor" in f.name]
            has_code_splitting = len(chunk_files) > 0
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å‹ç¼©
            sample_js = js_files[0] if js_files else None
            is_minified = False
            if sample_js:
                with open(sample_js, 'r', encoding='utf-8') as f:
                    content = f.read(1000)  # è¯»å–å‰1000å­—ç¬¦
                    is_minified = '\n' not in content and len(content.split()) < 10
            
            metrics = {
                "total_js_files": len(js_files),
                "total_css_files": len(css_files),
                "total_js_size_mb": round(total_js_size / 1024 / 1024, 2),
                "total_css_size_mb": round(total_css_size / 1024 / 1024, 2),
                "total_size_mb": round(total_size / 1024 / 1024, 2),
                "chunk_files_count": len(chunk_files),
                "has_code_splitting": has_code_splitting,
                "is_minified": is_minified
            }
            
            # è®¡ç®—æ”¹è¿›
            baseline_size = self.baseline_metrics["bundle_size"]
            current_size = total_size / 1024 / 1024  # MB
            size_improvement = ((baseline_size - current_size) / baseline_size) * 100
            
            # ä¼˜åŒ–æœ‰æ•ˆçš„æ ‡å‡†ï¼šæœ‰ä»£ç åˆ†å‰²ã€æœ‰å‹ç¼©ã€å¤§å°åˆç†
            optimization_effective = has_code_splitting and is_minified and current_size < baseline_size
            message = f"Bundleå¤§å°: {current_size:.2f}MB (æ”¹è¿›: {size_improvement:.1f}%), ä»£ç åˆ†å‰²: {has_code_splitting}, å‹ç¼©: {is_minified}"
            
            self.log_test("frontend_build_optimization", optimization_effective, metrics, message)
            
            # è®°å½•æ”¹è¿›æ•°æ®
            self.test_results["comparison"]["bundle_size"] = {
                "baseline": baseline_size,
                "current": current_size,
                "improvement": size_improvement,
                "target": baseline_size * 0.7,  # ç›®æ ‡å‡å°‘30%
                "target_met": current_size < baseline_size * 0.7
            }
            
            return optimization_effective
            
        except Exception as e:
            self.log_test("frontend_build_optimization", False, {}, f"å‰ç«¯æ„å»ºä¼˜åŒ–æµ‹è¯•å¼‚å¸¸: {str(e)}")
            return False
    
    def test_database_indexes(self):
        """æµ‹è¯•æ•°æ®åº“ç´¢å¼•æ•ˆæœ"""
        try:
            # æ£€æŸ¥ç´¢å¼•è¿ç§»æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            migration_file = Path("backend/migrations/004_add_performance_indexes.py")
            if not migration_file.exists():
                self.log_test("database_indexes", False, {}, "ç´¢å¼•è¿ç§»æ–‡ä»¶ä¸å­˜åœ¨")
                return False
            
            # å°è¯•æ‰§è¡Œç´¢å¼•åˆ›å»ºï¼ˆå¦‚æœè¿˜æ²¡æ‰§è¡Œï¼‰
            try:
                exec(open(migration_file).read())
                print("   ç´¢å¼•è¿ç§»å·²æ‰§è¡Œ")
            except Exception as e:
                print(f"   ç´¢å¼•è¿ç§»æ‰§è¡Œå¤±è´¥æˆ–å·²å­˜åœ¨: {e}")
            
            # æµ‹è¯•æŸ¥è¯¢æ€§èƒ½ï¼ˆç®€å•æµ‹è¯•ï¼‰
            query_times = []
            for i in range(5):
                start_time = time.time()
                try:
                    # æµ‹è¯•ä¸€ä¸ªåº”è¯¥ä½¿ç”¨ç´¢å¼•çš„æŸ¥è¯¢
                    response = requests.get(f"{self.base_url}/api/projects", timeout=10)
                    if response.status_code == 200:
                        query_time = (time.time() - start_time) * 1000
                        query_times.append(query_time)
                except Exception as e:
                    print(f"   æŸ¥è¯¢æµ‹è¯•å¤±è´¥: {e}")
            
            if query_times:
                avg_query_time = statistics.mean(query_times)
                baseline_query_time = self.baseline_metrics["database_query_time"]
                query_improvement = ((baseline_query_time - avg_query_time) / baseline_query_time) * 100
                
                metrics = {
                    "average_query_time_ms": round(avg_query_time, 2),
                    "query_improvement_percentage": round(query_improvement, 1),
                    "migration_file_exists": True,
                    "test_queries_count": len(query_times)
                }
                
                # ç´¢å¼•æœ‰æ•ˆçš„æ ‡å‡†ï¼šæŸ¥è¯¢æ—¶é—´æœ‰æ”¹è¿›
                indexes_effective = avg_query_time < baseline_query_time
                message = f"å¹³å‡æŸ¥è¯¢æ—¶é—´: {avg_query_time:.2f}ms (æ”¹è¿›: {query_improvement:.1f}%)"
                
                self.log_test("database_indexes", indexes_effective, metrics, message)
                
                # è®°å½•æ”¹è¿›æ•°æ®
                self.test_results["comparison"]["database_query_time"] = {
                    "baseline": baseline_query_time,
                    "current": avg_query_time,
                    "improvement": query_improvement,
                    "target": baseline_query_time * 0.5,  # ç›®æ ‡æå‡50%
                    "target_met": avg_query_time < baseline_query_time * 0.5
                }
                
                return indexes_effective
            else:
                self.log_test("database_indexes", False, {}, "æ•°æ®åº“æŸ¥è¯¢æµ‹è¯•å¤±è´¥")
                return False
                
        except Exception as e:
            self.log_test("database_indexes", False, {}, f"æ•°æ®åº“ç´¢å¼•æµ‹è¯•å¼‚å¸¸: {str(e)}")
            return False
    
    def calculate_overall_improvement(self):
        """è®¡ç®—æ€»ä½“æ€§èƒ½æ”¹è¿›"""
        try:
            improvements = {}
            
            # æ”¶é›†æ‰€æœ‰æ”¹è¿›æ•°æ®
            for metric, data in self.test_results["comparison"].items():
                if "improvement" in data:
                    improvements[metric] = data["improvement"]
            
            if improvements:
                avg_improvement = statistics.mean(improvements.values())
                self.test_results["summary"]["performance_improvements"] = {
                    "individual_improvements": improvements,
                    "average_improvement": round(avg_improvement, 1),
                    "total_metrics_improved": len([i for i in improvements.values() if i > 0])
                }
                
                print(f"\nğŸ“ˆ æ€»ä½“æ€§èƒ½æ”¹è¿›: {avg_improvement:.1f}%")
                for metric, improvement in improvements.items():
                    print(f"   {metric}: {improvement:.1f}%")
            
        except Exception as e:
            print(f"è®¡ç®—æ€»ä½“æ”¹è¿›å¤±è´¥: {e}")
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹P0/P1ä¿®å¤åæ€§èƒ½æµ‹è¯•")
        print("=" * 60)
        
        # 1. APIå“åº”æ—¶é—´æµ‹è¯•
        self.test_api_response_time()
        
        # 2. æ•°æ®åº“è¿æ¥æ± æµ‹è¯•
        self.test_database_connection_pool()
        
        # 3. ç¼“å­˜æœåŠ¡æµ‹è¯•
        self.test_cache_service()
        
        # 4. å“åº”å‹ç¼©æµ‹è¯•
        self.test_response_compression()
        
        # 5. å‰ç«¯æ„å»ºä¼˜åŒ–æµ‹è¯•
        self.test_frontend_build_optimization()
        
        # 6. æ•°æ®åº“ç´¢å¼•æµ‹è¯•
        self.test_database_indexes()
        
        # 7. è®¡ç®—æ€»ä½“æ”¹è¿›
        self.calculate_overall_improvement()
        
        # 8. è¾“å‡ºæµ‹è¯•ç»“æœ
        self.print_summary()
        self.save_results()
    
    def print_summary(self):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        print("\n" + "=" * 60)
        print("ğŸ“Š æ€§èƒ½æµ‹è¯•ç»“æœæ‘˜è¦")
        print("=" * 60)
        
        summary = self.test_results["summary"]
        print(f"æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
        print(f"é€šè¿‡: {summary['passed_tests']} âœ…")
        print(f"å¤±è´¥: {summary['failed_tests']} âŒ")
        print(f"æˆåŠŸç‡: {(summary['passed_tests']/summary['total_tests']*100):.1f}%")
        
        # æ˜¾ç¤ºæ€§èƒ½æ”¹è¿›
        if "performance_improvements" in summary:
            improvements = summary["performance_improvements"]
            print(f"\nğŸ¯ æ€§èƒ½æ”¹è¿›ç»Ÿè®¡:")
            print(f"å¹³å‡æ”¹è¿›: {improvements['average_improvement']}%")
            print(f"æ”¹è¿›æŒ‡æ ‡æ•°: {improvements['total_metrics_improved']}")
        
        # æ˜¾ç¤ºå¯¹æ¯”æ•°æ®
        if self.test_results["comparison"]:
            print(f"\nğŸ“ˆ å…³é”®æŒ‡æ ‡å¯¹æ¯”:")
            for metric, data in self.test_results["comparison"].items():
                baseline = data["baseline"]
                current = data["current"]
                improvement = data["improvement"]
                target_met = "âœ…" if data.get("target_met", False) else "âŒ"
                print(f"  {metric}: {baseline} â†’ {current:.2f} ({improvement:+.1f}%) {target_met}")
        
        print("\n" + "=" * 60)
    
    def save_results(self):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        try:
            filename = f"performance_test_report_{time.strftime('%Y%m%d_%H%M%S')}.json"
            with open(filename, "w", encoding="utf-8") as f:
                json.dump(self.test_results, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ“„ æ€§èƒ½æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {filename}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æµ‹è¯•æŠ¥å‘Šå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    tester = PerformanceTestSuite()
    tester.run_all_tests()

if __name__ == "__main__":
    main()