# -*- coding: utf-8 -*-
"""
Pervis PRO æ‰¹é‡ç´ æç´¢å¼• V2

Feature: pervis-asset-tagging
Task: 4.1 æ›´æ–°æ‰¹é‡ç´¢å¼•è„šæœ¬

æ–°åŠŸèƒ½ï¼š
1. é›†æˆå››çº§æ ‡ç­¾å±‚çº§ä½“ç³»
2. ä½¿ç”¨ Ollama åµŒå…¥æœåŠ¡
3. æ”¯æŒå¢é‡ç´¢å¼•
4. ç”Ÿæˆæ ‡ç­¾è¦†ç›–ç‡æŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•ï¼š
    cd "Pervis PRO"
    py batch_asset_indexing_v2.py --sample 300
    py batch_asset_indexing_v2.py --all --analyze
"""

import asyncio
import argparse
import hashlib
import json
import os
import re
import sys
import time
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

# åŠ è½½ç¯å¢ƒå˜é‡
def load_env():
    env_path = Path(__file__).parent / ".env"
    if env_path.exists():
        with open(env_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith("#") and "=" in line:
                    key, value = line.split("=", 1)
                    os.environ[key.strip()] = value.strip()

load_env()
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'backend'))

# ============================================================
# é…ç½®
# ============================================================

DAM_ASSET_ROOT = os.getenv("ASSET_ROOT", r"U:\PreVis_Assets")
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
LOCAL_MODEL = os.getenv("LOCAL_MODEL_NAME", "qwen2.5:7b")
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "nomic-embed-text")


# ============================================================
# æ ‡ç­¾ç”Ÿæˆå™¨ V2ï¼ˆä½¿ç”¨æ–°æ ‡ç­¾ä½“ç³»ï¼‰
# ============================================================

class TagGeneratorV2:
    """æ ‡ç­¾ç”Ÿæˆå™¨ V2 - ä½¿ç”¨å››çº§æ ‡ç­¾ä½“ç³»"""
    
    def __init__(self, base_url: str = OLLAMA_BASE_URL, model: str = LOCAL_MODEL):
        self.base_url = base_url
        self.model = model
    
    def extract_from_filename(self, filename: str, parent_dir: str = "") -> Dict[str, Any]:
        """ä»æ–‡ä»¶åå’Œç›®å½•åæå–æ ‡ç­¾"""
        from models.asset_tags import (
            SceneType, TimeOfDay, ShotSize, CameraMove, ActionType, Mood,
            KEYWORD_MAPPINGS, ANIME_KEYWORDS, CHARACTER_KEYWORDS, VFX_KEYWORDS
        )
        
        # æ¸…ç†æ–‡ä»¶å
        name = filename
        prefixes_to_remove = ["ã€å…è´¹æ›´æ–°+V Lingshao2605ã€‘", "ã€", "ã€‘"]
        for prefix in prefixes_to_remove:
            name = name.replace(prefix, "")
        name = Path(name).stem
        
        # åˆå¹¶æ–‡ä»¶åå’Œç›®å½•å
        full_text = f"{parent_dir} {name}"
        
        # åˆå§‹åŒ–æ ‡ç­¾
        tags = {
            # L1
            "scene_type": "UNKNOWN",
            "time_of_day": "UNKNOWN",
            "shot_size": "UNKNOWN",
            # L2
            "camera_move": "UNKNOWN",
            "action_type": "UNKNOWN",
            "mood": "UNKNOWN",
            # L3
            "characters": [],
            "props": [],
            "vfx": [],
            "environment": [],
            # L4
            "free_tags": [],
            "source_work": "",
            "summary": name[:50],
        }
        
        # ä»å…³é”®è¯æ˜ å°„æå–æ ‡ç­¾
        for field, value_keywords in KEYWORD_MAPPINGS.items():
            for value, keywords in value_keywords.items():
                if any(kw in full_text for kw in keywords):
                    tags[field] = value
                    break
        
        # è¯†åˆ«æ¥æºä½œå“
        for anime, keywords in ANIME_KEYWORDS.items():
            if any(kw in full_text for kw in keywords):
                tags["source_work"] = anime
                break
        
        # è¯†åˆ«è§’è‰²
        for char, keywords in CHARACTER_KEYWORDS.items():
            if any(kw in full_text for kw in keywords):
                if char not in tags["characters"]:
                    tags["characters"].append(char)
        
        # è¯†åˆ«ç‰¹æ•ˆ
        for vfx, keywords in VFX_KEYWORDS.items():
            if any(kw in full_text for kw in keywords):
                if vfx not in tags["vfx"]:
                    tags["vfx"].append(vfx)
        
        # æå–è‡ªç”±æ ‡ç­¾ï¼ˆä»æ–‡ä»¶ååˆ†è¯ï¼‰
        words = re.split(r'[\s_\-\.]+', name)
        free_tags = [w for w in words if len(w) > 1 and len(w) < 10][:10]
        tags["free_tags"] = free_tags
        
        return tags

    async def generate_with_llm(self, filename: str, parent_dir: str = "") -> Dict[str, Any]:
        """ä½¿ç”¨ LLM å¢å¼ºæ ‡ç­¾"""
        # å…ˆç”¨æ–‡ä»¶åæå–åŸºç¡€æ ‡ç­¾
        base_tags = self.extract_from_filename(filename, parent_dir)
        
        try:
            import aiohttp
            
            prompt = f"""åˆ†æä»¥ä¸‹è§†é¢‘ç´ æï¼Œç”Ÿæˆæ ‡ç­¾ã€‚

æ–‡ä»¶å: {filename}
ç›®å½•: {parent_dir}

è¯·è¿”å› JSON æ ¼å¼çš„æ ‡ç­¾ï¼š
{{
  "scene_type": "INT/EXT/INT-EXT/UNKNOWN",
  "time_of_day": "DAY/NIGHT/DAWN/DUSK/UNKNOWN",
  "shot_size": "ECU/CU/MCU/MS/MLS/LS/ELS/UNKNOWN",
  "camera_move": "STATIC/PAN/TILT/DOLLY/CRANE/HANDHELD/ZOOM/UNKNOWN",
  "action_type": "FIGHT/CHASE/DIALOGUE/IDLE/RUN/FLY/TRANSFORM/SKILL/UNKNOWN",
  "mood": "TENSE/SAD/HAPPY/CALM/HORROR/ROMANTIC/EPIC/NEUTRAL/UNKNOWN",
  "characters": ["è§’è‰²1", "è§’è‰²2"],
  "vfx": ["ç‰¹æ•ˆ1", "ç‰¹æ•ˆ2"],
  "environment": ["ç¯å¢ƒ1"],
  "source_work": "ä½œå“åç§°"
}}

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.base_url}/api/generate",
                    json={
                        "model": self.model,
                        "prompt": prompt,
                        "stream": False,
                        "options": {"temperature": 0.3}
                    },
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        response_text = data.get("response", "")
                        
                        # è§£æ JSON
                        json_match = re.search(r'\{[^{}]*\}', response_text, re.DOTALL)
                        if json_match:
                            llm_tags = json.loads(json_match.group())
                            # åˆå¹¶æ ‡ç­¾ï¼ˆLLM ç»“æœè¦†ç›–åŸºç¡€æ ‡ç­¾ï¼‰
                            for key in ["scene_type", "time_of_day", "shot_size", 
                                       "camera_move", "action_type", "mood", "source_work"]:
                                if key in llm_tags and llm_tags[key] not in ["UNKNOWN", "", None]:
                                    base_tags[key] = llm_tags[key]
                            for key in ["characters", "vfx", "environment"]:
                                if key in llm_tags and llm_tags[key]:
                                    base_tags[key] = list(set(base_tags.get(key, []) + llm_tags[key]))
        except Exception as e:
            pass  # ä½¿ç”¨åŸºç¡€æ ‡ç­¾
        
        return base_tags


# ============================================================
# ç´¢å¼•ç»Ÿè®¡
# ============================================================

@dataclass
class IndexingStatsV2:
    """ç´¢å¼•ç»Ÿè®¡ V2"""
    total_files: int = 0
    indexed: int = 0
    embedded: int = 0
    failed: int = 0
    start_time: datetime = field(default_factory=datetime.now)
    
    # å…³é”®å¸§å’Œè§†è§‰åµŒå…¥ç»Ÿè®¡
    keyframes_extracted: int = 0
    visual_embedded: int = 0
    
    # æ ‡ç­¾è¦†ç›–ç»Ÿè®¡
    tag_coverage: Dict[str, int] = field(default_factory=lambda: {
        "scene_type": 0, "time_of_day": 0, "shot_size": 0,
        "camera_move": 0, "action_type": 0, "mood": 0,
        "characters": 0, "vfx": 0, "environment": 0,
        "source_work": 0, "free_tags": 0,
    })
    
    def to_dict(self) -> Dict[str, Any]:
        elapsed = (datetime.now() - self.start_time).total_seconds()
        coverage_pct = {
            k: v / self.indexed * 100 if self.indexed > 0 else 0
            for k, v in self.tag_coverage.items()
        }
        return {
            "total_files": self.total_files,
            "indexed": self.indexed,
            "embedded": self.embedded,
            "failed": self.failed,
            "keyframes_extracted": self.keyframes_extracted,
            "visual_embedded": self.visual_embedded,
            "elapsed_seconds": elapsed,
            "rate": self.indexed / elapsed if elapsed > 0 else 0,
            "embedding_rate": self.embedded / self.indexed * 100 if self.indexed > 0 else 0,
            "keyframe_rate": self.keyframes_extracted / self.indexed * 100 if self.indexed > 0 else 0,
            "visual_embedding_rate": self.visual_embedded / self.indexed * 100 if self.indexed > 0 else 0,
            "tag_coverage_pct": coverage_pct,
        }


# ============================================================
# æ‰¹é‡ç´¢å¼•å™¨ V2
# ============================================================

class BatchAssetIndexerV2:
    """æ‰¹é‡ç´ æç´¢å¼•å™¨ V2"""
    
    VIDEO_EXTENSIONS = {'.mp4', '.mov', '.avi', '.mkv', '.wmv', '.flv', '.webm'}
    
    def __init__(
        self,
        asset_root: str = DAM_ASSET_ROOT,
        use_llm: bool = False,  # é»˜è®¤å…³é—­ LLMï¼ˆé€Ÿåº¦è€ƒè™‘ï¼‰
        use_embedding: bool = True,
        use_keyframes: bool = False,  # å…³é”®å¸§æå–
        use_visual_embedding: bool = False,  # CLIP è§†è§‰åµŒå…¥
    ):
        self.asset_root = asset_root
        self.use_llm = use_llm
        self.use_embedding = use_embedding
        self.use_keyframes = use_keyframes
        self.use_visual_embedding = use_visual_embedding
        
        self.tag_generator = TagGeneratorV2()
        self.embedding_service = None
        self.video_store = None
        self.keyframe_extractor = None
        self.clip_service = None
        self.visual_store = None
        self.stats = IndexingStatsV2()
        
        # ç´¢å¼•ç¼“å­˜
        self.cache_path = Path(__file__).parent / "data" / "index_cache_v2.json"
        self.index_cache: Dict[str, str] = {}
    
    def _load_cache(self):
        """åŠ è½½ç´¢å¼•ç¼“å­˜"""
        if self.cache_path.exists():
            try:
                with open(self.cache_path, "r", encoding="utf-8") as f:
                    self.index_cache = json.load(f)
                print(f"ğŸ“‚ å·²åŠ è½½ç´¢å¼•ç¼“å­˜: {len(self.index_cache)} æ¡è®°å½•")
            except:
                self.index_cache = {}
    
    def _save_cache(self):
        """ä¿å­˜ç´¢å¼•ç¼“å­˜"""
        self.cache_path.parent.mkdir(parents=True, exist_ok=True)
        with open(self.cache_path, "w", encoding="utf-8") as f:
            json.dump(self.index_cache, f, ensure_ascii=False, indent=2)
    
    def _get_file_hash(self, file_path: str) -> str:
        """è·å–æ–‡ä»¶å“ˆå¸Œ"""
        stat = os.stat(file_path)
        content = f"{file_path}:{stat.st_size}:{stat.st_mtime}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def scan_assets(self, sample_size: int = None, target_dirs: List[str] = None) -> List[Tuple[str, str]]:
        """æ‰«æç´ ææ–‡ä»¶ï¼Œè¿”å› (æ–‡ä»¶è·¯å¾„, çˆ¶ç›®å½•å) åˆ—è¡¨"""
        print(f"\nğŸ“ æ‰«æç´ æåº“: {self.asset_root}")
        
        video_files = []
        
        for root, dirs, files in os.walk(self.asset_root):
            if target_dirs:
                rel_path = os.path.relpath(root, self.asset_root)
                if not any(target in rel_path for target in target_dirs):
                    continue
            
            parent_dir = Path(root).name
            
            for file in files:
                ext = Path(file).suffix.lower()
                if ext in self.VIDEO_EXTENSIONS:
                    video_files.append((os.path.join(root, file), parent_dir))
        
        print(f"   æ‰¾åˆ° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
        
        if sample_size and sample_size < len(video_files):
            import random
            random.shuffle(video_files)
            video_files = video_files[:sample_size]
            print(f"   é‡‡æ · {sample_size} ä¸ªæ–‡ä»¶")
        
        self.stats.total_files = len(video_files)
        return video_files
    
    async def initialize(self):
        """åˆå§‹åŒ–æœåŠ¡"""
        from services.milvus_store import MemoryVideoStore
        from services.ollama_embedding import OllamaEmbeddingService
        
        # åˆå§‹åŒ–å­˜å‚¨
        self.video_store = MemoryVideoStore()
        await self.video_store.initialize()
        print("âœ… è§†é¢‘å­˜å‚¨åˆå§‹åŒ–å®Œæˆ")
        
        # åˆå§‹åŒ–åµŒå…¥æœåŠ¡
        if self.use_embedding:
            cache_path = str(Path(__file__).parent / "data" / "embedding_cache.json")
            self.embedding_service = OllamaEmbeddingService(
                model=EMBEDDING_MODEL,
                cache_path=cache_path
            )
            available, model = await self.embedding_service.check_available()
            if available:
                print(f"âœ… åµŒå…¥æœåŠ¡åˆå§‹åŒ–å®Œæˆ: {model} (ç»´åº¦: {self.embedding_service.dimension})")
            else:
                print("âš ï¸ åµŒå…¥æœåŠ¡ä¸å¯ç”¨ï¼Œå°†è·³è¿‡å‘é‡ç”Ÿæˆ")
                self.use_embedding = False
        
        # åˆå§‹åŒ–å…³é”®å¸§æå–å™¨
        if self.use_keyframes:
            try:
                from services.keyframe_extractor import KeyFrameExtractor
                from models.keyframe import KeyFrameConfig
                
                config = KeyFrameConfig(
                    strategy="hybrid",
                    max_frames=10,
                    interval_seconds=3.0,
                )
                self.keyframe_extractor = KeyFrameExtractor(config)
                print("âœ… å…³é”®å¸§æå–å™¨åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                print(f"âš ï¸ å…³é”®å¸§æå–å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.use_keyframes = False
        
        # åˆå§‹åŒ– CLIP è§†è§‰åµŒå…¥æœåŠ¡
        if self.use_visual_embedding:
            try:
                from services.clip_embedding import get_clip_service
                from services.visual_vector_store import get_visual_store
                
                self.clip_service = get_clip_service()
                await self.clip_service.initialize()
                
                self.visual_store = get_visual_store(
                    storage_path=str(Path(__file__).parent / "data" / "visual_vectors")
                )
                
                print(f"âœ… CLIP è§†è§‰åµŒå…¥æœåŠ¡åˆå§‹åŒ–å®Œæˆ: {self.clip_service.model_name} (ç»´åº¦: {self.clip_service.dimension})")
            except Exception as e:
                print(f"âš ï¸ CLIP è§†è§‰åµŒå…¥æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
                self.use_visual_embedding = False

    async def index_file(self, file_path: str, parent_dir: str, index: int) -> bool:
        """ç´¢å¼•å•ä¸ªæ–‡ä»¶"""
        from services.milvus_store import VideoSegment
        
        try:
            filename = Path(file_path).name
            file_hash = self._get_file_hash(file_path)
            segment_id = f"asset_{index:06d}"
            
            # æ£€æŸ¥ç¼“å­˜
            if file_hash in self.index_cache:
                return True
            
            # ç”Ÿæˆæ ‡ç­¾
            if self.use_llm:
                tags = await self.tag_generator.generate_with_llm(filename, parent_dir)
            else:
                tags = self.tag_generator.extract_from_filename(filename, parent_dir)
            
            # æ›´æ–°æ ‡ç­¾è¦†ç›–ç»Ÿè®¡
            for field in self.stats.tag_coverage:
                value = tags.get(field)
                if value and value not in ["UNKNOWN", "", None]:
                    if isinstance(value, list) and value:
                        self.stats.tag_coverage[field] += 1
                    elif not isinstance(value, list):
                        self.stats.tag_coverage[field] += 1
            
            # ç”ŸæˆåµŒå…¥å‘é‡
            embedding = None
            if self.use_embedding and self.embedding_service:
                # ç”Ÿæˆæœç´¢æ–‡æœ¬
                search_text = self._generate_search_text(tags, filename)
                embedding = await self.embedding_service.embed(search_text)
                if embedding:
                    self.stats.embedded += 1
            
            # æå–å…³é”®å¸§
            keyframes = []
            if self.use_keyframes and self.keyframe_extractor:
                try:
                    keyframes = await self.keyframe_extractor.extract(file_path)
                    if keyframes:
                        self.stats.keyframes_extracted += 1
                except Exception as e:
                    pass  # å…³é”®å¸§æå–å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
            
            # ç”Ÿæˆè§†è§‰åµŒå…¥
            if self.use_visual_embedding and self.clip_service and self.visual_store and keyframes:
                try:
                    visual_count = 0
                    for kf in keyframes:
                        if kf.image_path and os.path.exists(kf.image_path):
                            visual_vec = await self.clip_service.embed_image(kf.image_path)
                            if visual_vec:
                                self.visual_store.add(
                                    keyframe_id=f"{segment_id}_kf_{kf.frame_index:04d}",
                                    asset_id=segment_id,
                                    vector=visual_vec,
                                    frame_index=kf.frame_index,
                                    timestamp=kf.timestamp,
                                    timecode=kf.timecode,
                                    thumbnail_path=kf.image_path,
                                    metadata={"scene_id": kf.scene_id},
                                )
                                visual_count += 1
                    if visual_count > 0:
                        self.stats.visual_embedded += 1
                except Exception as e:
                    pass  # è§†è§‰åµŒå…¥å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
            
            # åˆ›å»ºè§†é¢‘ç‰‡æ®µ
            segment = VideoSegment(
                segment_id=segment_id,
                video_id=file_hash[:16],
                video_path=file_path,
                start_time=0,
                end_time=5.0,
                duration=5.0,
                tags=tags,
                embedding=embedding,
                description=tags.get("summary", filename[:50])
            )
            
            # å­˜å‚¨
            await self.video_store.insert(segment)
            
            # æ›´æ–°ç¼“å­˜
            self.index_cache[file_hash] = segment.segment_id
            self.stats.indexed += 1
            
            return True
            
        except Exception as e:
            self.stats.failed += 1
            print(f"   âŒ ç´¢å¼•å¤±è´¥ [{index}]: {e}")
            return False
    
    def _generate_search_text(self, tags: Dict[str, Any], filename: str) -> str:
        """ç”Ÿæˆç”¨äºå‘é‡æœç´¢çš„æ–‡æœ¬"""
        parts = []
        
        # L1/L2 æ ‡ç­¾
        for field in ["scene_type", "time_of_day", "action_type", "mood"]:
            value = tags.get(field)
            if value and value != "UNKNOWN":
                parts.append(value)
        
        # L3 æ ‡ç­¾
        for field in ["characters", "vfx", "environment"]:
            values = tags.get(field, [])
            if values:
                parts.extend(values)
        
        # L4 æ ‡ç­¾
        if tags.get("source_work"):
            parts.append(tags["source_work"])
        if tags.get("free_tags"):
            parts.extend(tags["free_tags"][:5])
        if tags.get("summary"):
            parts.append(tags["summary"])
        
        return ' '.join(parts) if parts else filename
    
    async def run(
        self,
        sample_size: int = None,
        target_dirs: List[str] = None
    ):
        """è¿è¡Œæ‰¹é‡ç´¢å¼•"""
        print("\n" + "="*70)
        print("Pervis PRO æ‰¹é‡ç´ æç´¢å¼• V2")
        print("="*70)
        
        self._load_cache()
        await self.initialize()
        
        video_files = self.scan_assets(sample_size, target_dirs)
        
        if not video_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°è§†é¢‘æ–‡ä»¶")
            return
        
        print(f"\nğŸš€ å¼€å§‹ç´¢å¼• {len(video_files)} ä¸ªæ–‡ä»¶...")
        print(f"   ä½¿ç”¨ LLM: {self.use_llm}")
        print(f"   ä½¿ç”¨åµŒå…¥: {self.use_embedding}")
        print(f"   ä½¿ç”¨å…³é”®å¸§: {self.use_keyframes}")
        print(f"   ä½¿ç”¨è§†è§‰åµŒå…¥: {self.use_visual_embedding}")
        print("-"*70)
        
        for i, (file_path, parent_dir) in enumerate(video_files):
            if (i + 1) % 20 == 0 or i == 0:
                progress = (i + 1) / len(video_files) * 100
                print(f"   [{i+1}/{len(video_files)}] {progress:.1f}%")
            
            await self.index_file(file_path, parent_dir, i)
            
            if (i + 1) % 100 == 0:
                self._save_cache()
                if self.embedding_service:
                    self.embedding_service.save_cache()
                if self.visual_store:
                    self.visual_store.save()
        
        self._save_cache()
        if self.embedding_service:
            self.embedding_service.save_cache()
        if self.visual_store:
            self.visual_store.save()
        
        # ä¿å­˜å®Œæ•´çš„ç´ ææ•°æ®åˆ°æ–°æ ¼å¼ç¼“å­˜ï¼ˆå…³é”®ä¿®å¤ï¼ï¼‰
        await self._save_segments_cache()
        
        self._print_stats()
        self._save_report()
    
    async def _save_segments_cache(self):
        """ä¿å­˜å®Œæ•´çš„ç´ ææ•°æ®åˆ°ç¼“å­˜æ–‡ä»¶ï¼ˆä¾›åç«¯å¯åŠ¨æ—¶åŠ è½½ï¼‰"""
        if self.video_store and hasattr(self.video_store, 'save_to_cache'):
            cache_path = Path(__file__).parent / "data" / "segments_cache.json"
            success = await self.video_store.save_to_cache(str(cache_path))
            if success:
                print(f"âœ… å·²ä¿å­˜å®Œæ•´ç´ ææ•°æ®åˆ° {cache_path}")
            else:
                print(f"âš ï¸ ä¿å­˜ç´ ææ•°æ®å¤±è´¥")
        else:
            # æ‰‹åŠ¨ä¿å­˜
            import json
            cache_path = Path(__file__).parent / "data" / "segments_cache.json"
            cache_path.parent.mkdir(parents=True, exist_ok=True)
            
            segments_data = []
            for segment in self.video_store._segments.values():
                seg_dict = segment.to_dict()
                seg_dict["embedding"] = segment.embedding
                segments_data.append(seg_dict)
            
            data = {
                "version": "2.0",
                "count": len(segments_data),
                "segments": segments_data
            }
            
            with open(cache_path, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False)
            
            print(f"âœ… å·²ä¿å­˜ {len(segments_data)} æ¡ç´ ææ•°æ®åˆ° {cache_path}")
    
    def _print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.to_dict()
        
        print("\n" + "="*70)
        print("ğŸ“Š ç´¢å¼•ç»Ÿè®¡")
        print("="*70)
        print(f"   æ€»æ–‡ä»¶æ•°: {stats['total_files']}")
        print(f"   å·²ç´¢å¼•: {stats['indexed']}")
        print(f"   å·²åµŒå…¥: {stats['embedded']} ({stats['embedding_rate']:.1f}%)")
        print(f"   å…³é”®å¸§æå–: {stats['keyframes_extracted']} ({stats['keyframe_rate']:.1f}%)")
        print(f"   è§†è§‰åµŒå…¥: {stats['visual_embedded']} ({stats['visual_embedding_rate']:.1f}%)")
        print(f"   å¤±è´¥: {stats['failed']}")
        print(f"   è€—æ—¶: {stats['elapsed_seconds']:.1f} ç§’")
        print(f"   é€Ÿç‡: {stats['rate']:.2f} æ–‡ä»¶/ç§’")
        
        print("\nğŸ“Š æ ‡ç­¾è¦†ç›–ç‡")
        print("-"*70)
        for field, pct in stats['tag_coverage_pct'].items():
            bar = "â–ˆ" * int(pct / 5) + "â–‘" * (20 - int(pct / 5))
            print(f"   {field:15s} {bar} {pct:.1f}%")
        print("="*70)
    
    def _save_report(self):
        """ä¿å­˜ç´¢å¼•æŠ¥å‘Š"""
        report_path = Path(__file__).parent / f"indexing_report_v2_{int(time.time())}.json"
        
        report = {
            "timestamp": datetime.now().isoformat(),
            "asset_root": self.asset_root,
            "stats": self.stats.to_dict(),
            "config": {
                "use_llm": self.use_llm,
                "use_embedding": self.use_embedding,
                "llm_model": LOCAL_MODEL,
                "embedding_model": EMBEDDING_MODEL
            }
        }
        
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        return report_path

    async def analyze_tags(self):
        """åˆ†ææ ‡ç­¾åˆ†å¸ƒ"""
        print("\n" + "="*70)
        print("ğŸ“Š æ ‡ç­¾åˆ†å¸ƒåˆ†æ")
        print("="*70)
        
        if not self.video_store:
            print("âŒ å­˜å‚¨æœªåˆå§‹åŒ–")
            return
        
        count = await self.video_store.count()
        print(f"   æ€»ç´ ææ•°: {count}")
        
        # ç»Ÿè®¡å„çº§æ ‡ç­¾
        l1_stats = {"scene_type": {}, "time_of_day": {}, "shot_size": {}}
        l2_stats = {"camera_move": {}, "action_type": {}, "mood": {}}
        l3_stats = {"characters": {}, "vfx": {}, "environment": {}}
        l4_stats = {"source_work": {}, "free_tags": {}}
        
        for segment_id, segment in self.video_store._segments.items():
            tags = segment.tags
            
            # L1 ç»Ÿè®¡
            for field in l1_stats:
                value = tags.get(field, "UNKNOWN")
                l1_stats[field][value] = l1_stats[field].get(value, 0) + 1
            
            # L2 ç»Ÿè®¡
            for field in l2_stats:
                value = tags.get(field, "UNKNOWN")
                l2_stats[field][value] = l2_stats[field].get(value, 0) + 1
            
            # L3 ç»Ÿè®¡
            for field in ["characters", "vfx", "environment"]:
                for value in tags.get(field, []):
                    l3_stats[field][value] = l3_stats[field].get(value, 0) + 1
            
            # L4 ç»Ÿè®¡
            if tags.get("source_work"):
                sw = tags["source_work"]
                l4_stats["source_work"][sw] = l4_stats["source_work"].get(sw, 0) + 1
            for tag in tags.get("free_tags", []):
                l4_stats["free_tags"][tag] = l4_stats["free_tags"].get(tag, 0) + 1
        
        # è¾“å‡º L1 ç»Ÿè®¡
        print("\nğŸ“Œ L1 ä¸€çº§æ ‡ç­¾ï¼ˆå¿…å¡«å•é€‰ï¼‰")
        for field, values in l1_stats.items():
            print(f"\n   {field}:")
            for value, cnt in sorted(values.items(), key=lambda x: x[1], reverse=True)[:5]:
                pct = cnt / count * 100 if count > 0 else 0
                print(f"      {value}: {cnt} ({pct:.1f}%)")
        
        # è¾“å‡º L2 ç»Ÿè®¡
        print("\nğŸ“Œ L2 äºŒçº§æ ‡ç­¾ï¼ˆå¿…å¡«å•é€‰ï¼‰")
        for field, values in l2_stats.items():
            print(f"\n   {field}:")
            for value, cnt in sorted(values.items(), key=lambda x: x[1], reverse=True)[:5]:
                pct = cnt / count * 100 if count > 0 else 0
                print(f"      {value}: {cnt} ({pct:.1f}%)")
        
        # è¾“å‡º L3 ç»Ÿè®¡
        print("\nğŸ“Œ L3 ä¸‰çº§æ ‡ç­¾ï¼ˆå¯é€‰å¤šé€‰ï¼‰Top 10")
        for field, values in l3_stats.items():
            if values:
                print(f"\n   {field}:")
                for value, cnt in sorted(values.items(), key=lambda x: x[1], reverse=True)[:10]:
                    print(f"      {value}: {cnt}")
        
        # è¾“å‡º L4 ç»Ÿè®¡
        print("\nğŸ“Œ L4 å››çº§æ ‡ç­¾ï¼ˆè‡ªç”±ï¼‰")
        if l4_stats["source_work"]:
            print("\n   source_work:")
            for value, cnt in sorted(l4_stats["source_work"].items(), key=lambda x: x[1], reverse=True)[:10]:
                print(f"      {value}: {cnt}")
        
        if l4_stats["free_tags"]:
            print("\n   free_tags (Top 20):")
            for value, cnt in sorted(l4_stats["free_tags"].items(), key=lambda x: x[1], reverse=True)[:20]:
                print(f"      {value}: {cnt}")


# ============================================================
# æœç´¢æµ‹è¯•
# ============================================================

async def test_search(indexer: BatchAssetIndexerV2):
    """æµ‹è¯•æœç´¢åŠŸèƒ½"""
    from services.search_service import HybridSearchService, SearchRequest, SearchMode
    
    print("\n" + "="*70)
    print("ğŸ” æœç´¢æµ‹è¯•")
    print("="*70)
    
    search_service = HybridSearchService(
        video_store=indexer.video_store,
        embedding_service=indexer.embedding_service
    )
    
    test_cases = [
        {"query": "ç‚­æ²»éƒæˆ˜æ–—", "tags": {"action_type": "FIGHT"}},
        {"query": "å–„é€¸é›·ä¹‹å‘¼å¸", "tags": {"characters": ["å–„é€¸"]}},
        {"query": "çƒ­è¡€æ‰“æ–—åœºé¢", "tags": {"mood": "EPIC", "action_type": "FIGHT"}},
        {"query": "å¤œæ™šæ£®æ—", "tags": {"time_of_day": "NIGHT"}},
    ]
    
    for i, tc in enumerate(test_cases):
        print(f"\næµ‹è¯• {i+1}: query=\"{tc['query']}\" tags={tc['tags']}")
        
        request = SearchRequest(
            query=tc["query"],
            tags=tc["tags"],
            mode=SearchMode.HYBRID,
            top_k=3
        )
        
        response = await search_service.search(request)
        
        print(f"   ç»“æœæ•°: {response.total}, è€—æ—¶: {response.search_time_ms:.1f}ms")
        for j, r in enumerate(response.results):
            print(f"   [{j+1}] score={r.score:.3f} - {Path(r.video_path).name[:40]}...")


# ============================================================
# ä¸»å‡½æ•°
# ============================================================

async def main():
    parser = argparse.ArgumentParser(description="Pervis PRO æ‰¹é‡ç´ æç´¢å¼• V2")
    parser.add_argument("--sample", type=int, default=300, help="é‡‡æ ·æ•°é‡ï¼ˆé»˜è®¤300ï¼‰")
    parser.add_argument("--all", action="store_true", help="ç´¢å¼•æ‰€æœ‰æ–‡ä»¶")
    parser.add_argument("--llm", action="store_true", help="ä½¿ç”¨ LLM å¢å¼ºæ ‡ç­¾")
    parser.add_argument("--no-embedding", action="store_true", help="ä¸ç”ŸæˆåµŒå…¥å‘é‡")
    parser.add_argument("--keyframes", action="store_true", help="æå–å…³é”®å¸§")
    parser.add_argument("--visual", action="store_true", help="ç”Ÿæˆ CLIP è§†è§‰åµŒå…¥")
    parser.add_argument("--dirs", nargs="+", help="æŒ‡å®šç›®å½•")
    parser.add_argument("--analyze", action="store_true", help="åˆ†ææ ‡ç­¾åˆ†å¸ƒ")
    parser.add_argument("--test-search", action="store_true", help="æµ‹è¯•æœç´¢åŠŸèƒ½")
    
    args = parser.parse_args()
    
    sample_size = None if args.all else args.sample
    
    indexer = BatchAssetIndexerV2(
        use_llm=args.llm,
        use_embedding=not args.no_embedding,
        use_keyframes=args.keyframes,
        use_visual_embedding=args.visual,
    )
    
    await indexer.run(
        sample_size=sample_size,
        target_dirs=args.dirs
    )
    
    if args.analyze:
        await indexer.analyze_tags()
    
    if args.test_search:
        await test_search(indexer)


if __name__ == "__main__":
    asyncio.run(main())
