# -*- coding: utf-8 -*-
"""
Pervis PRO æ‰¹é‡ç´ æç´¢å¼•å’Œæ‰“æ ‡è„šæœ¬

åŠŸèƒ½ï¼š
1. æ‰«æ DAM ç´ æåº“
2. ä½¿ç”¨ Ollama æœ¬åœ°æ¨¡å‹ç”Ÿæˆæ ‡ç­¾
3. ä½¿ç”¨ Ollama åµŒå…¥æ¨¡å‹ç”Ÿæˆå‘é‡ï¼ˆç»•è¿‡ NumPy é—®é¢˜ï¼‰
4. å­˜å‚¨åˆ°å†…å­˜/Milvus

ä½¿ç”¨æ–¹æ³•ï¼š
    cd "Pervis PRO"
    py batch_asset_indexing.py --sample 200
    py batch_asset_indexing.py --all
"""

import asyncio
import argparse
import json
import os
import sys
import time
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field
import hashlib

# åŠ è½½ç¯å¢ƒå˜é‡
def load_env():
    env_path = Path(__file__).parent / ".env"
    if env_path.exists():
        with open(env_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith("#") and "=" in line:
                    key, value = line.split("=", 1)
                    os.environ[key.strip()] = value.strip()

load_env()
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'backend'))

# ============================================================
# é…ç½®
# ============================================================

DAM_ASSET_ROOT = os.getenv("ASSET_ROOT", r"U:\PreVis_Assets")
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
LOCAL_MODEL = os.getenv("LOCAL_MODEL_NAME", "qwen2.5:7b")
EMBEDDING_MODEL = "nomic-embed-text"  # Ollama åµŒå…¥æ¨¡å‹

# æ ‡ç­¾ç±»å‹å®šä¹‰
TAG_CATEGORIES = {
    "scene_type": ["å®¤å†…", "å®¤å¤–", "æ··åˆ"],
    "time": ["ç™½å¤©", "å¤œæ™š", "é»„æ˜", "é»æ˜", "æœªçŸ¥"],
    "shot_type": ["ç‰¹å†™", "è¿‘æ™¯", "ä¸­æ™¯", "å…¨æ™¯", "è¿œæ™¯", "ä¿¯æ‹", "ä»°æ‹"],
    "mood": ["ç´§å¼ ", "æ‚²ä¼¤", "æ¬¢ä¹", "å¹³é™", "ææ€–", "æµªæ¼«", "çƒ­è¡€", "æœªçŸ¥"],
    "action": ["æ‰“æ–—", "è¿½é€", "å¯¹è¯", "é™æ€", "å¥”è·‘", "é£è¡Œ", "å˜èº«", "æŠ€èƒ½é‡Šæ”¾"],
}

# åŠ¨æ¼«å…³é”®è¯æ˜ å°„
ANIME_KEYWORDS = {
    # é¬¼ç­ä¹‹åˆƒ
    "é¬¼ç­": ["é¬¼ç­ä¹‹åˆƒ", "ç‚­æ²»éƒ", "å¼¥è±†å­", "å–„é€¸", "ä¼Šä¹‹åŠ©", "é¬¼æ€é˜Ÿ"],
    "ç‚­æ²»éƒ": ["æ°´ä¹‹å‘¼å¸", "æ—¥è½®åˆ€", "ä¸»è§’"],
    "å–„é€¸": ["é›·ä¹‹å‘¼å¸", "éœ¹é›³ä¸€é—ª", "ç¡çœ æˆ˜æ–—"],
    "å¼¥è±†å­": ["è¡€é¬¼æœ¯", "é¬¼åŒ–", "å¦¹å¦¹"],
    "ä¹‰å‹‡": ["æ°´æŸ±", "æ°´ä¹‹å‘¼å¸"],
    "èœ˜è››": ["èœ˜è››é¬¼", "é‚£ç”°èœ˜è››å±±"],
    
    # åŠ¨ä½œç±»å‹
    "æˆ˜æ–—": ["æ‰“æ–—", "æˆ˜æ–—", "æ”»å‡»", "æŠ€èƒ½"],
    "ç ": ["æ–©å‡»", "åˆ€", "å‰‘"],
    "å†²åˆº": ["å†²åˆº", "å¿«é€Ÿç§»åŠ¨", "é—ªé¿"],
    "æ—‹è½¬": ["æ—‹è½¬", "å›æ—‹", "è½¬èº«"],
    "é£": ["é£è¡Œ", "è·³è·ƒ", "ç©ºä¸­"],
    "è½åœ°": ["è½åœ°", "ç€é™†"],
    
    # åœºæ™¯
    "æ£®æ—": ["æ£®æ—", "æ ‘æ—", "å®¤å¤–"],
    "å±‹": ["å®¤å†…", "æˆ¿é—´", "å»ºç­‘"],
    "å¤œ": ["å¤œæ™š", "é»‘æš—", "æœˆå…‰"],
}


# ============================================================
# Ollama åµŒå…¥æœåŠ¡ï¼ˆç»•è¿‡ NumPy é—®é¢˜ï¼‰
# ============================================================

class OllamaEmbedding:
    """ä½¿ç”¨ Ollama ç”ŸæˆåµŒå…¥å‘é‡"""
    
    def __init__(self, base_url: str = OLLAMA_BASE_URL, model: str = EMBEDDING_MODEL):
        self.base_url = base_url
        self.model = model
        self._available = None
    
    async def check_available(self) -> bool:
        """æ£€æŸ¥åµŒå…¥æ¨¡å‹æ˜¯å¦å¯ç”¨"""
        if self._available is not None:
            return self._available
        
        try:
            import aiohttp
            async with aiohttp.ClientSession() as session:
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å®‰è£…
                async with session.get(f"{self.base_url}/api/tags") as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        models = [m.get("name", "") for m in data.get("models", [])]
                        self._available = any(self.model in m for m in models)
                        if not self._available:
                            print(f"âš ï¸ åµŒå…¥æ¨¡å‹ {self.model} æœªå®‰è£…")
                            print(f"   å¯ç”¨æ¨¡å‹: {models}")
                            print(f"   å®‰è£…å‘½ä»¤: ollama pull {self.model}")
                        return self._available
        except Exception as e:
            print(f"âš ï¸ Ollama æœåŠ¡ä¸å¯ç”¨: {e}")
            self._available = False
        return False
    
    async def embed(self, text: str) -> Optional[List[float]]:
        """ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡"""
        if not await self.check_available():
            return None
        
        try:
            import aiohttp
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.base_url}/api/embeddings",
                    json={"model": self.model, "prompt": text},
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        return data.get("embedding")
        except Exception as e:
            print(f"âš ï¸ ç”ŸæˆåµŒå…¥å¤±è´¥: {e}")
        return None
    
    async def embed_batch(self, texts: List[str]) -> List[Optional[List[float]]]:
        """æ‰¹é‡ç”ŸæˆåµŒå…¥å‘é‡"""
        results = []
        for text in texts:
            embedding = await self.embed(text)
            results.append(embedding)
        return results


# ============================================================
# æ ‡ç­¾ç”Ÿæˆå™¨
# ============================================================

class TagGenerator:
    """æ ‡ç­¾ç”Ÿæˆå™¨"""
    
    def __init__(self, base_url: str = OLLAMA_BASE_URL, model: str = LOCAL_MODEL):
        self.base_url = base_url
        self.model = model
    
    def extract_from_filename(self, filename: str) -> Dict[str, Any]:
        """ä»æ–‡ä»¶åæå–æ ‡ç­¾"""
        # æ¸…ç†æ–‡ä»¶å
        name = filename
        # ç§»é™¤å¸¸è§å‰ç¼€
        prefixes_to_remove = [
            "ã€å…è´¹æ›´æ–°+V Lingshao2605ã€‘",
            "ã€",
            "ã€‘",
        ]
        for prefix in prefixes_to_remove:
            name = name.replace(prefix, "")
        
        # ç§»é™¤æ‰©å±•å
        name = Path(name).stem
        
        tags = {
            "scene_type": "æœªçŸ¥",
            "time": "æœªçŸ¥",
            "shot_type": "æœªçŸ¥",
            "mood": "æœªçŸ¥",
            "action": "é™æ€",
            "characters": [],
            "free_tags": [],
            "source_anime": "æœªçŸ¥",
            "summary": name[:100]
        }
        
        # æå–å…³é”®è¯
        keywords = []
        for word in re.split(r'[\s_\-\.]+', name):
            if len(word) > 1:
                keywords.append(word)
        
        # åŒ¹é…åŠ¨æ¼«å…³é”®è¯
        matched_tags = set()
        for keyword in keywords:
            for key, related in ANIME_KEYWORDS.items():
                if key in keyword or keyword in key:
                    matched_tags.update(related)
                    if key in ["é¬¼ç­", "ç‚­æ²»éƒ", "å–„é€¸", "å¼¥è±†å­", "ä¹‰å‹‡"]:
                        tags["source_anime"] = "é¬¼ç­ä¹‹åˆƒ"
        
        # è¯†åˆ«åœºæ™¯ç±»å‹
        if any(k in name for k in ["å®¤å†…", "æˆ¿é—´", "å±‹", "è½¦å¢", "é¤å…"]):
            tags["scene_type"] = "å®¤å†…"
        elif any(k in name for k in ["å®¤å¤–", "æ£®æ—", "è¡—é“", "å¤©ç©º", "å±±", "æµ·"]):
            tags["scene_type"] = "å®¤å¤–"
        
        # è¯†åˆ«æ—¶é—´
        if any(k in name for k in ["å¤œ", "æœˆ", "é»‘æš—"]):
            tags["time"] = "å¤œæ™š"
        elif any(k in name for k in ["æ—¥", "é˜³å…‰", "ç™½å¤©"]):
            tags["time"] = "ç™½å¤©"
        elif any(k in name for k in ["é»„æ˜", "å¤•é˜³"]):
            tags["time"] = "é»„æ˜"
        
        # è¯†åˆ«é•œå¤´ç±»å‹
        if any(k in name for k in ["ç‰¹å†™", "è„¸", "çœ¼"]):
            tags["shot_type"] = "ç‰¹å†™"
        elif any(k in name for k in ["å…¨æ™¯", "è¿œæ™¯", "å…¨èº«"]):
            tags["shot_type"] = "å…¨æ™¯"
        elif any(k in name for k in ["è¿‘æ™¯"]):
            tags["shot_type"] = "è¿‘æ™¯"
        
        # è¯†åˆ«åŠ¨ä½œ
        if any(k in name for k in ["æˆ˜æ–—", "æ‰“æ–—", "ç ", "æ–©", "æ”»å‡»", "æŠ€èƒ½"]):
            tags["action"] = "æ‰“æ–—"
            tags["mood"] = "ç´§å¼ "
        elif any(k in name for k in ["è·‘", "è¿½", "é€ƒ", "å†²åˆº"]):
            tags["action"] = "è¿½é€"
        elif any(k in name for k in ["é£", "è·³", "ç©ºä¸­"]):
            tags["action"] = "é£è¡Œ"
        elif any(k in name for k in ["æ—‹è½¬", "è½¬èº«"]):
            tags["action"] = "æŠ€èƒ½é‡Šæ”¾"
        
        # è¯†åˆ«æƒ…ç»ª
        if any(k in name for k in ["ç‡ƒ", "çƒ­è¡€", "æˆ˜æ–—", "æ€’"]):
            tags["mood"] = "çƒ­è¡€"
        elif any(k in name for k in ["å“­", "æ³ª", "æ‚²"]):
            tags["mood"] = "æ‚²ä¼¤"
        elif any(k in name for k in ["ç¬‘", "æç¬‘", "æ¬¢ä¹"]):
            tags["mood"] = "æ¬¢ä¹"
        
        # è¯†åˆ«è§’è‰²
        character_names = ["ç‚­æ²»éƒ", "å–„é€¸", "ä¼Šä¹‹åŠ©", "å¼¥è±†å­", "ä¹‰å‹‡", "è´è¶å¿", "ç…¤ç‚­éƒ"]
        for char in character_names:
            if char in name:
                tags["characters"].append(char)
        
        # åˆå¹¶æ ‡ç­¾
        tags["free_tags"] = list(matched_tags)[:10]
        if not tags["free_tags"]:
            tags["free_tags"] = keywords[:10]
        
        return tags
    
    async def generate_with_llm(self, filename: str, description: str = "") -> Dict[str, Any]:
        """ä½¿ç”¨ LLM ç”Ÿæˆæ ‡ç­¾"""
        # å…ˆç”¨æ–‡ä»¶åæå–åŸºç¡€æ ‡ç­¾
        base_tags = self.extract_from_filename(filename)
        
        # å¦‚æœæœ‰ LLMï¼Œå°è¯•å¢å¼ºæ ‡ç­¾
        try:
            import aiohttp
            
            prompt = f"""åˆ†æä»¥ä¸‹è§†é¢‘ç´ ææ–‡ä»¶åï¼Œç”Ÿæˆæ ‡ç­¾ã€‚

æ–‡ä»¶å: {filename}
{f'æè¿°: {description}' if description else ''}

è¯·è¿”å› JSON æ ¼å¼ï¼š
{{
  "scene_type": "å®¤å†…/å®¤å¤–/æ··åˆ",
  "time": "ç™½å¤©/å¤œæ™š/é»„æ˜/é»æ˜",
  "shot_type": "ç‰¹å†™/è¿‘æ™¯/ä¸­æ™¯/å…¨æ™¯/è¿œæ™¯",
  "mood": "ç´§å¼ /æ‚²ä¼¤/æ¬¢ä¹/å¹³é™/çƒ­è¡€",
  "action": "æ‰“æ–—/è¿½é€/å¯¹è¯/é™æ€/é£è¡Œ/æŠ€èƒ½é‡Šæ”¾",
  "characters": ["è§’è‰²1", "è§’è‰²2"],
  "free_tags": ["æ ‡ç­¾1", "æ ‡ç­¾2", "æ ‡ç­¾3"]
}}

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.base_url}/api/generate",
                    json={
                        "model": self.model,
                        "prompt": prompt,
                        "stream": False,
                        "options": {"temperature": 0.3}
                    },
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        response_text = data.get("response", "")
                        
                        # å°è¯•è§£æ JSON
                        try:
                            # æå– JSON éƒ¨åˆ†
                            json_match = re.search(r'\{[^{}]*\}', response_text, re.DOTALL)
                            if json_match:
                                llm_tags = json.loads(json_match.group())
                                # åˆå¹¶æ ‡ç­¾
                                for key in ["scene_type", "time", "shot_type", "mood", "action"]:
                                    if key in llm_tags and llm_tags[key] != "æœªçŸ¥":
                                        base_tags[key] = llm_tags[key]
                                if "characters" in llm_tags:
                                    base_tags["characters"] = list(set(base_tags.get("characters", []) + llm_tags["characters"]))
                                if "free_tags" in llm_tags:
                                    base_tags["free_tags"] = list(set(base_tags.get("free_tags", []) + llm_tags["free_tags"]))[:15]
                        except json.JSONDecodeError:
                            pass
                            
        except Exception as e:
            pass  # ä½¿ç”¨åŸºç¡€æ ‡ç­¾
        
        return base_tags


# ============================================================
# æ‰¹é‡ç´¢å¼•å™¨
# ============================================================

@dataclass
class IndexingStats:
    """ç´¢å¼•ç»Ÿè®¡"""
    total_files: int = 0
    indexed: int = 0
    tagged: int = 0
    embedded: int = 0
    failed: int = 0
    start_time: datetime = field(default_factory=datetime.now)
    
    def to_dict(self) -> Dict[str, Any]:
        elapsed = (datetime.now() - self.start_time).total_seconds()
        return {
            "total_files": self.total_files,
            "indexed": self.indexed,
            "tagged": self.tagged,
            "embedded": self.embedded,
            "failed": self.failed,
            "elapsed_seconds": elapsed,
            "rate": self.indexed / elapsed if elapsed > 0 else 0
        }


class BatchAssetIndexer:
    """æ‰¹é‡ç´ æç´¢å¼•å™¨"""
    
    VIDEO_EXTENSIONS = {'.mp4', '.mov', '.avi', '.mkv', '.wmv', '.flv', '.webm'}
    
    def __init__(
        self,
        asset_root: str = DAM_ASSET_ROOT,
        use_llm: bool = True,
        use_embedding: bool = True
    ):
        self.asset_root = asset_root
        self.use_llm = use_llm
        self.use_embedding = use_embedding
        
        self.tag_generator = TagGenerator()
        self.embedding_service = OllamaEmbedding()
        self.video_store = None
        self.stats = IndexingStats()
        
        # ç´¢å¼•ç¼“å­˜ï¼ˆé¿å…é‡å¤ç´¢å¼•ï¼‰
        self.index_cache_path = Path(__file__).parent / "data" / "index_cache.json"
        self.index_cache: Dict[str, str] = {}  # {file_hash: segment_id}
    
    def _load_cache(self):
        """åŠ è½½ç´¢å¼•ç¼“å­˜"""
        if self.index_cache_path.exists():
            try:
                with open(self.index_cache_path, "r", encoding="utf-8") as f:
                    self.index_cache = json.load(f)
                print(f"ğŸ“‚ å·²åŠ è½½ç´¢å¼•ç¼“å­˜: {len(self.index_cache)} æ¡è®°å½•")
            except:
                self.index_cache = {}
    
    def _save_cache(self):
        """ä¿å­˜ç´¢å¼•ç¼“å­˜"""
        self.index_cache_path.parent.mkdir(parents=True, exist_ok=True)
        with open(self.index_cache_path, "w", encoding="utf-8") as f:
            json.dump(self.index_cache, f, ensure_ascii=False, indent=2)
    
    def _get_file_hash(self, file_path: str) -> str:
        """è·å–æ–‡ä»¶å“ˆå¸Œï¼ˆç”¨äºå»é‡ï¼‰"""
        # ä½¿ç”¨æ–‡ä»¶è·¯å¾„å’Œå¤§å°ç”Ÿæˆå“ˆå¸Œ
        stat = os.stat(file_path)
        content = f"{file_path}:{stat.st_size}:{stat.st_mtime}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def scan_assets(self, sample_size: int = None, target_dirs: List[str] = None) -> List[str]:
        """æ‰«æç´ ææ–‡ä»¶"""
        print(f"\nğŸ“ æ‰«æç´ æåº“: {self.asset_root}")
        
        video_files = []
        
        for root, dirs, files in os.walk(self.asset_root):
            # å¦‚æœæŒ‡å®šäº†ç›®æ ‡ç›®å½•ï¼Œåªæ‰«æè¿™äº›ç›®å½•
            if target_dirs:
                rel_path = os.path.relpath(root, self.asset_root)
                if not any(target in rel_path for target in target_dirs):
                    continue
            
            for file in files:
                ext = Path(file).suffix.lower()
                if ext in self.VIDEO_EXTENSIONS:
                    video_files.append(os.path.join(root, file))
        
        print(f"   æ‰¾åˆ° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
        
        # é‡‡æ ·
        if sample_size and sample_size < len(video_files):
            import random
            random.shuffle(video_files)
            video_files = video_files[:sample_size]
            print(f"   é‡‡æ · {sample_size} ä¸ªæ–‡ä»¶")
        
        self.stats.total_files = len(video_files)
        return video_files
    
    async def initialize_store(self):
        """åˆå§‹åŒ–å­˜å‚¨"""
        from services.milvus_store import get_video_store, VectorStoreType, MemoryVideoStore
        
        # ä½¿ç”¨å†…å­˜å­˜å‚¨ï¼ˆMilvus éœ€è¦ Dockerï¼‰
        self.video_store = MemoryVideoStore()
        await self.video_store.initialize()
        print("âœ… è§†é¢‘å­˜å‚¨åˆå§‹åŒ–å®Œæˆ")
    
    async def index_file(self, file_path: str, index: int) -> bool:
        """ç´¢å¼•å•ä¸ªæ–‡ä»¶"""
        try:
            filename = Path(file_path).name
            file_hash = self._get_file_hash(file_path)
            
            # æ£€æŸ¥ç¼“å­˜
            if file_hash in self.index_cache:
                return True
            
            # ç”Ÿæˆæ ‡ç­¾
            if self.use_llm:
                tags = await self.tag_generator.generate_with_llm(filename)
            else:
                tags = self.tag_generator.extract_from_filename(filename)
            
            self.stats.tagged += 1
            
            # ç”ŸæˆåµŒå…¥å‘é‡
            embedding = None
            if self.use_embedding:
                # ä½¿ç”¨æ ‡ç­¾å’Œæ–‡ä»¶åç”Ÿæˆæè¿°
                description = f"{tags.get('summary', '')} {' '.join(tags.get('free_tags', []))}"
                embedding = await self.embedding_service.embed(description)
                if embedding:
                    self.stats.embedded += 1
            
            # åˆ›å»ºè§†é¢‘ç‰‡æ®µ
            from services.milvus_store import VideoSegment
            segment = VideoSegment(
                segment_id=f"asset_{index:06d}",
                video_id=file_hash[:16],
                video_path=file_path,
                start_time=0,
                end_time=5.0,  # é»˜è®¤æ—¶é•¿
                duration=5.0,
                tags=tags,
                embedding=embedding,
                description=tags.get("summary", filename)
            )
            
            # å­˜å‚¨
            await self.video_store.insert(segment)
            
            # æ›´æ–°ç¼“å­˜
            self.index_cache[file_hash] = segment.segment_id
            self.stats.indexed += 1
            
            return True
            
        except Exception as e:
            self.stats.failed += 1
            print(f"   âŒ ç´¢å¼•å¤±è´¥ [{index}]: {e}")
            return False
    
    async def run(
        self,
        sample_size: int = None,
        target_dirs: List[str] = None,
        batch_size: int = 10
    ):
        """è¿è¡Œæ‰¹é‡ç´¢å¼•"""
        print("\n" + "="*70)
        print("Pervis PRO æ‰¹é‡ç´ æç´¢å¼•")
        print("="*70)
        
        # åŠ è½½ç¼“å­˜
        self._load_cache()
        
        # åˆå§‹åŒ–å­˜å‚¨
        await self.initialize_store()
        
        # æ£€æŸ¥åµŒå…¥æœåŠ¡
        if self.use_embedding:
            available = await self.embedding_service.check_available()
            if not available:
                print("âš ï¸ åµŒå…¥æœåŠ¡ä¸å¯ç”¨ï¼Œå°†è·³è¿‡å‘é‡ç”Ÿæˆ")
                self.use_embedding = False
        
        # æ‰«ææ–‡ä»¶
        video_files = self.scan_assets(sample_size, target_dirs)
        
        if not video_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°è§†é¢‘æ–‡ä»¶")
            return
        
        # æ‰¹é‡ç´¢å¼•
        print(f"\nğŸš€ å¼€å§‹ç´¢å¼• {len(video_files)} ä¸ªæ–‡ä»¶...")
        print(f"   ä½¿ç”¨ LLM: {self.use_llm}")
        print(f"   ä½¿ç”¨åµŒå…¥: {self.use_embedding}")
        print("-"*70)
        
        for i, file_path in enumerate(video_files):
            # è¿›åº¦æ˜¾ç¤º
            if (i + 1) % 10 == 0 or i == 0:
                progress = (i + 1) / len(video_files) * 100
                print(f"   [{i+1}/{len(video_files)}] {progress:.1f}% - {Path(file_path).name[:40]}...")
            
            await self.index_file(file_path, i)
            
            # å®šæœŸä¿å­˜ç¼“å­˜
            if (i + 1) % 50 == 0:
                self._save_cache()
        
        # æœ€ç»ˆä¿å­˜
        self._save_cache()
        
        # è¾“å‡ºç»Ÿè®¡
        self._print_stats()
        self._save_report()
    
    def _print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.to_dict()
        
        print("\n" + "="*70)
        print("ğŸ“Š ç´¢å¼•ç»Ÿè®¡")
        print("="*70)
        print(f"   æ€»æ–‡ä»¶æ•°: {stats['total_files']}")
        print(f"   å·²ç´¢å¼•: {stats['indexed']}")
        print(f"   å·²æ‰“æ ‡: {stats['tagged']}")
        print(f"   å·²åµŒå…¥: {stats['embedded']}")
        print(f"   å¤±è´¥: {stats['failed']}")
        print(f"   è€—æ—¶: {stats['elapsed_seconds']:.1f} ç§’")
        print(f"   é€Ÿç‡: {stats['rate']:.2f} æ–‡ä»¶/ç§’")
        print("="*70)
    
    def _save_report(self):
        """ä¿å­˜ç´¢å¼•æŠ¥å‘Š"""
        report_path = Path(__file__).parent / f"indexing_report_{int(time.time())}.json"
        
        report = {
            "timestamp": datetime.now().isoformat(),
            "asset_root": self.asset_root,
            "stats": self.stats.to_dict(),
            "config": {
                "use_llm": self.use_llm,
                "use_embedding": self.use_embedding,
                "llm_model": LOCAL_MODEL,
                "embedding_model": EMBEDDING_MODEL
            }
        }
        
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    async def analyze_tags(self):
        """åˆ†ææ ‡ç­¾åˆ†å¸ƒ"""
        print("\n" + "="*70)
        print("ğŸ“Š æ ‡ç­¾åˆ†å¸ƒåˆ†æ")
        print("="*70)
        
        if not self.video_store:
            print("âŒ å­˜å‚¨æœªåˆå§‹åŒ–")
            return
        
        # ç»Ÿè®¡æ ‡ç­¾
        tag_stats = {
            "scene_type": {},
            "time": {},
            "shot_type": {},
            "mood": {},
            "action": {},
            "source_anime": {},
            "free_tags": {}
        }
        
        count = await self.video_store.count()
        print(f"   æ€»ç´ ææ•°: {count}")
        
        # éå†æ‰€æœ‰ç´ æ
        for segment_id, segment in self.video_store._segments.items():
            tags = segment.tags
            
            for category in ["scene_type", "time", "shot_type", "mood", "action", "source_anime"]:
                value = tags.get(category, "æœªçŸ¥")
                tag_stats[category][value] = tag_stats[category].get(value, 0) + 1
            
            for tag in tags.get("free_tags", []):
                tag_stats["free_tags"][tag] = tag_stats["free_tags"].get(tag, 0) + 1
        
        # è¾“å‡ºç»Ÿè®¡
        for category, values in tag_stats.items():
            if category == "free_tags":
                # åªæ˜¾ç¤º Top 20
                sorted_tags = sorted(values.items(), key=lambda x: x[1], reverse=True)[:20]
                print(f"\n   {category} (Top 20):")
                for tag, cnt in sorted_tags:
                    print(f"      {tag}: {cnt}")
            else:
                print(f"\n   {category}:")
                for value, cnt in sorted(values.items(), key=lambda x: x[1], reverse=True):
                    pct = cnt / count * 100 if count > 0 else 0
                    print(f"      {value}: {cnt} ({pct:.1f}%)")


# ============================================================
# ä¸»å‡½æ•°
# ============================================================

async def main():
    parser = argparse.ArgumentParser(description="Pervis PRO æ‰¹é‡ç´ æç´¢å¼•")
    parser.add_argument("--sample", type=int, default=200, help="é‡‡æ ·æ•°é‡ï¼ˆé»˜è®¤200ï¼‰")
    parser.add_argument("--all", action="store_true", help="ç´¢å¼•æ‰€æœ‰æ–‡ä»¶")
    parser.add_argument("--no-llm", action="store_true", help="ä¸ä½¿ç”¨ LLM ç”Ÿæˆæ ‡ç­¾")
    parser.add_argument("--no-embedding", action="store_true", help="ä¸ç”ŸæˆåµŒå…¥å‘é‡")
    parser.add_argument("--dirs", nargs="+", help="æŒ‡å®šç›®å½•ï¼ˆå¦‚ é¬¼ç­ æ‰“æ–—ï¼‰")
    parser.add_argument("--analyze", action="store_true", help="åˆ†ææ ‡ç­¾åˆ†å¸ƒ")
    
    args = parser.parse_args()
    
    sample_size = None if args.all else args.sample
    
    indexer = BatchAssetIndexer(
        use_llm=not args.no_llm,
        use_embedding=not args.no_embedding
    )
    
    await indexer.run(
        sample_size=sample_size,
        target_dirs=args.dirs
    )
    
    if args.analyze:
        await indexer.analyze_tags()


if __name__ == "__main__":
    asyncio.run(main())
